{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e069d60-ab88-4402-9480-d18ef495aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Multi-Modal Learning Model\n",
    "class MultiModalNetwork(nn.Module):\n",
    "    def __init__(self, input_size_audio=None, input_size_visual=None, hidden_size=128, output_size=6):\n",
    "        super(MultiModalNetwork, self).__init__()\n",
    "        \n",
    "        self.has_audio = input_size_audio is not None\n",
    "        self.has_visual = input_size_visual is not None\n",
    "        \n",
    "        if self.has_audio:\n",
    "            self.audio_feature_extractor = nn.Linear(input_size_audio, hidden_size)\n",
    "            self.specific_audio_classifier = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        if self.has_visual:\n",
    "            self.visual_feature_extractor = nn.Linear(input_size_visual, hidden_size)\n",
    "            self.specific_visual_classifier = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        if self.has_audio and self.has_visual:\n",
    "            self.common_classifier = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, audio_input=None, visual_input=None):\n",
    "        audio_features = self.audio_feature_extractor(audio_input) if self.has_audio and audio_input is not None else None\n",
    "        visual_features = self.visual_feature_extractor(visual_input) if self.has_visual and visual_input is not None else None\n",
    "\n",
    "        common_features = None\n",
    "        if self.has_audio and self.has_visual and audio_features is not None and visual_features is not None:\n",
    "            common_features = (audio_features + visual_features) / 2\n",
    "            specific_audio_features = audio_features - common_features\n",
    "            specific_visual_features = visual_features - common_features\n",
    "        else:\n",
    "            specific_audio_features = audio_features\n",
    "            specific_visual_features = visual_features\n",
    "        \n",
    "        common_pred = self.common_classifier(common_features) if common_features is not None else None\n",
    "        audio_pred = self.specific_audio_classifier(specific_audio_features) if self.has_audio and specific_audio_features is not None else None\n",
    "        visual_pred = self.specific_visual_classifier(specific_visual_features) if self.has_visual and specific_visual_features is not None else None\n",
    "        \n",
    "        return common_pred, audio_pred, visual_pred, audio_features, visual_features, common_features, specific_audio_features, specific_visual_features\n",
    "\n",
    "def similarity_loss(common_features, common_classifier, mask):\n",
    "    loss = 0.0\n",
    "    count = 0.0\n",
    "    for modality_set in mask:\n",
    "        if len(modality_set) < 2:\n",
    "            continue\n",
    "        modality_list = list(modality_set)\n",
    "        print(modality_list)\n",
    "        Dk = common_features[modality_list[0]].shape[0]\n",
    "        all_pairs = [(m1, m2) for i, m1 in enumerate(modality_list) for m2 in modality_list[i + 1:]]\n",
    "        count += len(all_pairs) * Dk\n",
    "\n",
    "        for m1, m2 in all_pairs:\n",
    "            for d in range(Dk):\n",
    "                h_m1 = common_features[m1][d].unsqueeze(0)\n",
    "                h_m2 = common_features[m2][d].unsqueeze(0)\n",
    "                z1 = F.softmax(common_classifier(h_m1), dim=1)\n",
    "                z2 = F.softmax(common_classifier(h_m2), dim=1)\n",
    "                loss += F.kl_div(z1.log(), z2, reduction='sum')\n",
    "\n",
    "    if count > 0:\n",
    "        loss = loss / count\n",
    "    else:\n",
    "        loss = torch.tensor(0.0, device=loss.device)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def auxiliary_classification_loss(common_features, common_classifier, labels, mask):\n",
    "    loss = 0.0\n",
    "    count = 0.0\n",
    "    for modality_set in mask:\n",
    "        modality_list = list(modality_set)\n",
    "        Dk = common_features[modality_list[0]].shape[0]\n",
    "        count += len(modality_list) * Dk\n",
    "\n",
    "        for m in modality_list:\n",
    "            for d in range(Dk):\n",
    "                h_m = common_features[m][d].unsqueeze(0)\n",
    "                y_m = labels[m][d].unsqueeze(0)\n",
    "                pred = F.softmax(common_classifier(h_m), dim=1)\n",
    "                loss += F.cross_entropy(pred, y_m)\n",
    "\n",
    "    if count > 0:\n",
    "        loss = loss / count\n",
    "    else:\n",
    "        loss = torch.tensor(0.0, device=loss.device)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def difference_loss(common_features, specific_features, mask):\n",
    "    loss = 0.0\n",
    "    for modality_set in mask:\n",
    "        for m in modality_set:\n",
    "            if common_features[m] is not None and specific_features[m] is not None:\n",
    "                loss += torch.norm(torch.matmul(common_features[m].T, specific_features[m]), p='fro')**2\n",
    "    return loss\n",
    "\n",
    "def feature_decomposition_loss(common_features, specific_features, common_classifier, labels, mask, alpha1, alpha2, alpha3):\n",
    "    sim_loss = similarity_loss(common_features, common_classifier, mask)\n",
    "    cls_loss = auxiliary_classification_loss(common_features, common_classifier, labels, mask)\n",
    "    diff_loss = difference_loss(common_features, specific_features, mask)\n",
    "    total_loss = alpha1 * sim_loss + alpha2 * cls_loss + alpha3 * diff_loss\n",
    "    return total_loss\n",
    "\n",
    "# Local Training Function\n",
    "def train_local(model, devices, num_epochs=5, alpha1=1.0, alpha2=1.0, alpha3=1.0):\n",
    "    for device_id, device_data in devices.items():\n",
    "        local_model = model\n",
    "        optimizer = optim.Adam(local_model.parameters(), lr=0.001)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            inputs_audio, inputs_visual, labels = device_data['data']\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            common_pred, audio_pred, visual_pred, audio_features, visual_features, common_features, specific_audio_features, specific_visual_features = local_model(inputs_audio, inputs_visual)\n",
    "            print(\"Shape of common_pred:\", common_pred.shape)\n",
    "            print(\"Shape of audio_pred:\", audio_pred.shape)\n",
    "            print(\"Shape of visual_pred:\", visual_pred.shape)\n",
    "            print(\"Shape of audio_features:\", audio_features.shape)\n",
    "            print(\"Shape of visual_features:\", visual_features.shape)\n",
    "            print(\"Shape of common_features:\", common_features.shape)\n",
    "            print(\"Shape of specific_audio_features:\", specific_audio_features.shape)\n",
    "            print(\"Shape of specific_visual_features:\", specific_visual_features.shape)\n",
    "            print()\n",
    "            loss = 0\n",
    "            mask = []\n",
    "            if audio_features is not None:\n",
    "                if visual_features is not None:\n",
    "                    mask.append({'audio', 'visual'})\n",
    "                else:\n",
    "                    mask.append({'audio'})\n",
    "            elif visual_features is not None:\n",
    "                mask.append({'visual'})\n",
    "\n",
    "            features = {\"audio\": audio_features, \"visual\": visual_features}\n",
    "            specifics = {\"audio\": specific_audio_features, \"visual\": specific_visual_features}\n",
    "            labels_dict = {\"audio\": labels['audio'], \"visual\": labels['visual']}\n",
    "\n",
    "            # Compute loss\n",
    "            feature_loss = feature_decomposition_loss(common_features, specifics, local_model.common_classifier, labels_dict, mask, alpha1, alpha2, alpha3)\n",
    "            total_loss = feature_loss\n",
    "            total_loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "        # After training on the device, print final model loss (optional)\n",
    "        print(f\"Device {device_id} training completed\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2760dc2-a98a-4d89-81d7-843dad4a3b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of common_pred: torch.Size([100, 6])\n",
      "Shape of audio_pred: torch.Size([100, 6])\n",
      "Shape of visual_pred: torch.Size([100, 6])\n",
      "Shape of audio_features: torch.Size([100, 128])\n",
      "Shape of visual_features: torch.Size([100, 128])\n",
      "Shape of common_features: torch.Size([100, 128])\n",
      "Shape of specific_audio_features: torch.Size([100, 128])\n",
      "Shape of specific_visual_features: torch.Size([100, 128])\n",
      "\n",
      "['visual', 'audio']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Number of local training epochs per device\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Train the model locally on each device\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[43mtrain_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[44], line 143\u001b[0m, in \u001b[0;36mtrain_local\u001b[1;34m(model, devices, num_epochs, alpha1, alpha2, alpha3)\u001b[0m\n\u001b[0;32m    140\u001b[0m labels_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: labels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisual\u001b[39m\u001b[38;5;124m\"\u001b[39m: labels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvisual\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m feature_loss \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_decomposition_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommon_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecifics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommon_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m feature_loss\n\u001b[0;32m    145\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[1;32mIn[44], line 102\u001b[0m, in \u001b[0;36mfeature_decomposition_loss\u001b[1;34m(common_features, specific_features, common_classifier, labels, mask, alpha1, alpha2, alpha3)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_decomposition_loss\u001b[39m(common_features, specific_features, common_classifier, labels, mask, alpha1, alpha2, alpha3):\n\u001b[1;32m--> 102\u001b[0m     sim_loss \u001b[38;5;241m=\u001b[39m \u001b[43msimilarity_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommon_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommon_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     cls_loss \u001b[38;5;241m=\u001b[39m auxiliary_classification_loss(common_features, common_classifier, labels, mask)\n\u001b[0;32m    104\u001b[0m     diff_loss \u001b[38;5;241m=\u001b[39m difference_loss(common_features, specific_features, mask)\n",
      "Cell \u001b[1;32mIn[44], line 52\u001b[0m, in \u001b[0;36msimilarity_loss\u001b[1;34m(common_features, common_classifier, mask)\u001b[0m\n\u001b[0;32m     50\u001b[0m modality_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(modality_set)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(modality_list)\n\u001b[1;32m---> 52\u001b[0m Dk \u001b[38;5;241m=\u001b[39m \u001b[43mcommon_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodality_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     53\u001b[0m all_pairs \u001b[38;5;241m=\u001b[39m [(m1, m2) \u001b[38;5;28;01mfor\u001b[39;00m i, m1 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modality_list) \u001b[38;5;28;01mfor\u001b[39;00m m2 \u001b[38;5;129;01min\u001b[39;00m modality_list[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:]]\n\u001b[0;32m     54\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_pairs) \u001b[38;5;241m*\u001b[39m Dk\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "# Initialize Model Parameters\n",
    "input_size_audio = 256  # Example size, modify according to your data\n",
    "input_size_visual = 256  # Example size, modify according to your data\n",
    "hidden_size = 128  # Hidden size of the network\n",
    "output_size = 6  # Number of output classes\n",
    "\n",
    "# Initialize Models\n",
    "model = MultiModalNetwork(input_size_audio=input_size_audio, input_size_visual=input_size_visual, hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "# Setup Devices (simulating multiple devices in a decentralized setup)\n",
    "# This should be a dictionary where each device has its local data and epochs.\n",
    "# Example setup:\n",
    "devices = {\n",
    "    0: {'data': (torch.randn(100, input_size_audio), torch.randn(100, input_size_visual), {'audio': torch.randint(0, 6, (100,)), 'visual': torch.randint(0, 6, (100,))}), 'local_epochs': 5},\n",
    "    1: {'data': (torch.randn(100, input_size_audio), torch.randn(100, input_size_visual), {'audio': torch.randint(0, 6, (100,)), 'visual': torch.randint(0, 6, (100,))}), 'local_epochs': 5},\n",
    "    2: {'data': (torch.randn(100, input_size_audio), torch.randn(100, input_size_visual), {'audio': torch.randint(0, 6, (100,)), 'visual': torch.randint(0, 6, (100,))}), 'local_epochs': 5},\n",
    "}\n",
    "\n",
    "\n",
    "# Training Hyperparameters\n",
    "num_epochs = 5  # Number of local training epochs per device\n",
    "\n",
    "# Train the model locally on each device\n",
    "train_local(model=model, devices=devices, num_epochs=num_epochs)\n",
    "\n",
    "# The model is now trained locally on each device without aggregation or knowledge distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "977dac22-3ff7-4d02-93de-31f479aa6b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Prediction:  tensor([[-0.3139,  0.8427, -0.2166, -0.2099,  0.0227, -0.2840],\n",
      "        [ 0.2369,  0.7433, -0.9447,  0.8359, -0.3473,  0.7672],\n",
      "        [-0.5883,  0.7150, -0.7838,  0.4420,  0.7502,  0.1397],\n",
      "        [-0.2176,  0.3576, -1.3419,  0.4045, -0.7248, -0.1803],\n",
      "        [ 0.9697, -0.0764,  1.4621, -1.8019, -0.8623, -0.1169],\n",
      "        [ 0.2682,  1.1872,  0.6554,  0.1557,  1.2513,  0.4830],\n",
      "        [-0.0960, -0.5458, -0.7141, -0.6288, -1.0434,  0.5657],\n",
      "        [ 0.5358,  0.6907, -0.9732, -0.7303, -1.0202, -0.6421],\n",
      "        [-0.3643,  0.6754, -0.4350, -0.2614,  0.7190, -0.8152],\n",
      "        [ 0.3813,  0.5324,  1.5932, -0.2552, -0.5837, -0.2788]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Audio Common Prediction:  tensor([[-3.9049e-01,  3.7859e-01, -1.0550e-01,  4.4970e-02,  5.3332e-02,\n",
      "         -1.0326e-01],\n",
      "        [ 2.9309e-01, -8.4598e-02, -6.6774e-01,  1.5795e-01,  1.8329e-01,\n",
      "         -8.3928e-02],\n",
      "        [-1.5160e-01, -1.9393e-01, -7.3705e-01, -1.5110e-02, -9.0311e-02,\n",
      "         -8.3477e-01],\n",
      "        [ 2.4474e-01,  3.0686e-01,  2.6997e-01,  3.9287e-02, -1.2956e-01,\n",
      "         -9.1359e-02],\n",
      "        [-7.6281e-02,  1.4720e-01, -3.7674e-02, -1.0119e+00, -2.1430e-01,\n",
      "         -7.6651e-01],\n",
      "        [ 7.3692e-04,  3.2119e-01, -2.0283e-01,  3.2429e-01,  3.2133e-01,\n",
      "         -9.6053e-02],\n",
      "        [ 3.0236e-01, -3.7174e-01, -1.8813e-01, -4.6031e-01, -4.2169e-01,\n",
      "          4.4164e-01],\n",
      "        [-1.0813e-01,  7.4865e-02, -4.8502e-01,  8.0412e-02, -4.2396e-01,\n",
      "          1.4903e-01],\n",
      "        [-3.7686e-01, -1.8045e-01, -7.8573e-03,  2.1743e-01,  4.4331e-01,\n",
      "         -6.5147e-01],\n",
      "        [ 1.1596e-01,  5.1269e-01,  4.0278e-01, -2.2183e-01, -1.0284e-01,\n",
      "          3.5309e-01]], grad_fn=<AddmmBackward0>)\n",
      "Audio Specific Prediction:  tensor([[-0.3892,  0.5966, -0.0656,  0.2682,  0.1017, -0.1829],\n",
      "        [-0.0688,  0.6005, -0.7716, -0.0219, -0.5929,  0.3028],\n",
      "        [ 0.0585,  0.0508, -0.0715,  0.2303,  0.4302,  0.2189],\n",
      "        [-0.1172,  0.6220, -0.4516,  0.1791, -0.0367, -0.0599],\n",
      "        [ 0.3839, -0.3115,  0.2919, -0.0965,  0.0580,  0.4332],\n",
      "        [ 0.0922,  0.2981,  0.3985,  0.3489,  0.1777,  0.3390],\n",
      "        [-0.0819, -0.2691, -0.1425, -0.3253,  0.2614, -0.1958],\n",
      "        [ 0.2613,  1.3049,  0.1090, -0.1503, -0.4533,  0.1989],\n",
      "        [ 0.3623,  0.1861,  0.1756,  0.2545,  0.4586,  0.2702],\n",
      "        [ 0.3968,  0.1486, -0.0519, -0.0044, -0.3039, -0.5964]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Visual Common Prediction:  tensor([[-0.0847, -0.4304, -0.1859, -0.2642, -0.1646,  0.3566],\n",
      "        [-0.3351,  0.1595,  0.6717,  0.2712, -0.3900,  0.3711],\n",
      "        [-0.0128,  0.9727,  0.2580,  0.2801,  0.4227,  0.1235],\n",
      "        [-0.0225, -0.2950, -0.3047, -0.1739, -0.3971, -0.0882],\n",
      "        [ 0.3323, -0.1046,  0.6070, -0.1890, -0.6876,  0.2111],\n",
      "        [-0.0491,  0.2722, -0.0742, -0.3096,  0.4003,  0.2925],\n",
      "        [-0.3311,  0.1868, -0.3162,  0.0029, -0.6543, -0.1162],\n",
      "        [-0.2710, -0.5431, -0.5668, -0.1878, -0.1178, -0.9130],\n",
      "        [-0.4845, -0.1507, -0.1606, -0.5027,  0.0413, -0.7272],\n",
      "        [-0.3194, -0.5706,  0.8053,  0.5189, -0.0629, -0.3848]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Visual Specific Prediction:  tensor([[ 0.5506,  0.2979,  0.1403, -0.2589,  0.0323, -0.3545],\n",
      "        [ 0.3478,  0.0679, -0.1771,  0.4287,  0.4523,  0.1772],\n",
      "        [-0.4825, -0.1145, -0.2333, -0.0533, -0.0124,  0.6321],\n",
      "        [-0.3227, -0.2763, -0.8556,  0.3600, -0.1614,  0.0591],\n",
      "        [ 0.3297,  0.1925,  0.6009, -0.5045, -0.0185,  0.0053],\n",
      "        [ 0.2244,  0.2957,  0.5339, -0.2078,  0.3520, -0.0524],\n",
      "        [ 0.0146, -0.0918, -0.0673,  0.1540, -0.2288,  0.4359],\n",
      "        [ 0.6536, -0.1459, -0.0303, -0.4726, -0.0251, -0.0770],\n",
      "        [ 0.1348,  0.8205, -0.4422, -0.2307, -0.2242,  0.2933],\n",
      "        [ 0.1880,  0.4416,  0.4370, -0.5478, -0.1139,  0.3493]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Shape of Combined Prediction: torch.Size([10, 6])\n",
      "Shape of Audio Common Prediction: torch.Size([10, 6])\n",
      "Shape of Audio Specific Prediction: torch.Size([10, 6])\n",
      "Shape of Visual Common Prediction: torch.Size([10, 6])\n",
      "Shape of Visual Specific Prediction: torch.Size([10, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiModalNetwork(nn.Module):\n",
    "    def __init__(self, input_size_audio=None, input_size_visual=None, hidden_size=128, output_size=6):\n",
    "        super(MultiModalNetwork, self).__init__()\n",
    "        \n",
    "        self.has_audio = input_size_audio is not None\n",
    "        self.has_visual = input_size_visual is not None\n",
    "        \n",
    "        if self.has_audio:\n",
    "            # Audio feature extractor\n",
    "            self.audio_feature_extractor = nn.Linear(input_size_audio, hidden_size)\n",
    "            # Split audio output into common and specific features\n",
    "            self.audio_common_classifier = nn.Linear(hidden_size // 2, output_size)  # Common classifier\n",
    "            self.audio_specific_classifier = nn.Linear(hidden_size // 2, output_size)  # Specific classifier\n",
    "        \n",
    "        if self.has_visual:\n",
    "            # Visual feature extractor\n",
    "            self.visual_feature_extractor = nn.Linear(input_size_visual, hidden_size)\n",
    "            # Split visual output into common and specific features\n",
    "            self.visual_common_classifier = nn.Linear(hidden_size // 2, output_size)  # Common classifier\n",
    "            self.visual_specific_classifier = nn.Linear(hidden_size // 2, output_size)  # Specific classifier\n",
    "        \n",
    "        # Common classifier shared by both modalities\n",
    "        self.common_classifier = nn.Linear(hidden_size // 2, output_size)  # Same size for common features\n",
    "    \n",
    "    def forward(self, audio_input=None, visual_input=None):\n",
    "        audio_features = self.audio_feature_extractor(audio_input) if self.has_audio and audio_input is not None else None\n",
    "        visual_features = self.visual_feature_extractor(visual_input) if self.has_visual and visual_input is not None else None\n",
    "\n",
    "        common_audio_features = None\n",
    "        common_visual_features = None\n",
    "        specific_audio_features = None\n",
    "        specific_visual_features = None\n",
    "        \n",
    "        if self.has_audio and audio_features is not None:\n",
    "            # Split audio features into common and specific parts\n",
    "            common_audio_features = audio_features[:, :audio_features.size(1) // 2]\n",
    "            specific_audio_features = audio_features[:, audio_features.size(1) // 2:]\n",
    "            audio_common_pred = self.audio_common_classifier(common_audio_features)\n",
    "            audio_specific_pred = self.audio_specific_classifier(specific_audio_features)\n",
    "        \n",
    "        if self.has_visual and visual_features is not None:\n",
    "            # Split visual features into common and specific parts\n",
    "            common_visual_features = visual_features[:, :visual_features.size(1) // 2]\n",
    "            specific_visual_features = visual_features[:, visual_features.size(1) // 2:]\n",
    "            visual_common_pred = self.visual_common_classifier(common_visual_features)\n",
    "            visual_specific_pred = self.visual_specific_classifier(specific_visual_features)\n",
    "\n",
    "        # Combine the common and specific predictions with a bias term\n",
    "        final_audio_pred = audio_common_pred + audio_specific_pred if audio_features is not None else None\n",
    "        final_visual_pred = visual_common_pred + visual_specific_pred if visual_features is not None else None\n",
    "        \n",
    "        # If both modalities are present, combine the final predictions\n",
    "        combined_pred = None\n",
    "        if final_audio_pred is not None and final_visual_pred is not None:\n",
    "            combined_pred = final_audio_pred + final_visual_pred\n",
    "        elif final_audio_pred is not None:\n",
    "            combined_pred = final_audio_pred\n",
    "        elif final_visual_pred is not None:\n",
    "            combined_pred = final_visual_pred\n",
    "        \n",
    "        return combined_pred, audio_common_pred, audio_specific_pred, visual_common_pred, visual_specific_pred\n",
    "\n",
    "# Initialize Model Parameters\n",
    "input_size_audio = 256  # Example size, modify according to your data\n",
    "input_size_visual = 256  # Example size, modify according to your data\n",
    "hidden_size = 128  # Hidden size of the network\n",
    "output_size = 6  # Number of output classes\n",
    "\n",
    "# Initialize the MultiModalNetwork model\n",
    "model = MultiModalNetwork(input_size_audio=input_size_audio, input_size_visual=input_size_visual, hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "# Example input data\n",
    "audio_input = torch.randn(10, input_size_audio)  # Batch of 10 audio samples\n",
    "visual_input = torch.randn(10, input_size_visual)  # Batch of 10 visual samples\n",
    "\n",
    "# Forward pass through the model\n",
    "combined_pred, audio_common_pred, audio_specific_pred, visual_common_pred, visual_specific_pred = model(audio_input, visual_input)\n",
    "\n",
    "print(\"Combined Prediction: \", combined_pred)\n",
    "print(\"Audio Common Prediction: \", audio_common_pred)\n",
    "print(\"Audio Specific Prediction: \", audio_specific_pred)\n",
    "print(\"Visual Common Prediction: \", visual_common_pred)\n",
    "print(\"Visual Specific Prediction: \", visual_specific_pred)\n",
    "\n",
    "print(\"Shape of Combined Prediction:\", combined_pred.shape)\n",
    "print(\"Shape of Audio Common Prediction:\", audio_common_pred.shape)\n",
    "print(\"Shape of Audio Specific Prediction:\", audio_specific_pred.shape)\n",
    "print(\"Shape of Visual Common Prediction:\", visual_common_pred.shape)\n",
    "print(\"Shape of Visual Specific Prediction:\", visual_specific_pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fa510e63-6492-4bd3-b25c-4ae11add4af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss: tensor(9519.3359, grad_fn=<AddBackward0>)\n",
      "Similarity Loss: tensor(0.1491, grad_fn=<DivBackward0>)\n",
      "Auxiliary Classification Loss: tensor(3.4934, grad_fn=<AddBackward0>)\n",
      "Difference Loss: tensor(9515.6934, grad_fn=<AddBackward0>)\n",
      "\n",
      "When only audio is present:\n",
      "Total Loss: tensor(4791.9634, grad_fn=<AddBackward0>)\n",
      "Similarity Loss: 0.0\n",
      "Auxiliary Classification Loss: tensor(1.7387, grad_fn=<AddBackward0>)\n",
      "Difference Loss: tensor(4790.2246, grad_fn=<AddBackward0>)\n",
      "\n",
      "When only visual is present:\n",
      "Total Loss: tensor(4727.2241, grad_fn=<AddBackward0>)\n",
      "Similarity Loss: 0.0\n",
      "Auxiliary Classification Loss: tensor(1.7547, grad_fn=<AddBackward0>)\n",
      "Difference Loss: tensor(4725.4692, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiModalNetwork(nn.Module):\n",
    "    def __init__(self, input_size_audio=None, input_size_visual=None, hidden_size=128, output_size=6):\n",
    "        super(MultiModalNetwork, self).__init__()\n",
    "        \n",
    "        self.has_audio = input_size_audio is not None\n",
    "        self.has_visual = input_size_visual is not None\n",
    "        \n",
    "        if self.has_audio:\n",
    "            # Audio feature extractor\n",
    "            self.audio_feature_extractor = nn.Linear(input_size_audio, hidden_size)\n",
    "            # Split audio output into common and specific features\n",
    "            self.audio_common_classifier = nn.Linear(hidden_size // 2, output_size)  # Common classifier\n",
    "            self.audio_specific_classifier = nn.Linear(hidden_size // 2, output_size)  # Specific classifier\n",
    "        \n",
    "        if self.has_visual:\n",
    "            # Visual feature extractor\n",
    "            self.visual_feature_extractor = nn.Linear(input_size_visual, hidden_size)\n",
    "            # Split visual output into common and specific features\n",
    "            self.visual_common_classifier = nn.Linear(hidden_size // 2, output_size)  # Common classifier\n",
    "            self.visual_specific_classifier = nn.Linear(hidden_size // 2, output_size)  # Specific classifier\n",
    "        \n",
    "        # Common classifier shared by both modalities\n",
    "        self.common_classifier = nn.Linear(hidden_size // 2, output_size)  # Same size for common features\n",
    "\n",
    "    def forward(self, audio_input=None, visual_input=None):\n",
    "        audio_features = self.audio_feature_extractor(audio_input) if self.has_audio and audio_input is not None else None\n",
    "        visual_features = self.visual_feature_extractor(visual_input) if self.has_visual and visual_input is not None else None\n",
    "\n",
    "        common_audio_features = None\n",
    "        common_visual_features = None\n",
    "        specific_audio_features = None\n",
    "        specific_visual_features = None\n",
    "        \n",
    "        if self.has_audio and audio_features is not None:\n",
    "            # Split audio features into common and specific parts\n",
    "            common_audio_features = audio_features[:, :audio_features.size(1) // 2]\n",
    "            specific_audio_features = audio_features[:, audio_features.size(1) // 2:]\n",
    "            audio_common_pred = self.audio_common_classifier(common_audio_features)\n",
    "            audio_specific_pred = self.audio_specific_classifier(specific_audio_features)\n",
    "        else:\n",
    "            audio_common_pred = None\n",
    "            audio_specific_pred = None\n",
    "        \n",
    "        if self.has_visual and visual_features is not None:\n",
    "            # Split visual features into common and specific parts\n",
    "            common_visual_features = visual_features[:, :visual_features.size(1) // 2]\n",
    "            specific_visual_features = visual_features[:, visual_features.size(1) // 2:]\n",
    "            visual_common_pred = self.visual_common_classifier(common_visual_features)\n",
    "            visual_specific_pred = self.visual_specific_classifier(specific_visual_features)\n",
    "        else:\n",
    "            visual_common_pred = None\n",
    "            visual_specific_pred = None\n",
    "\n",
    "        # Combine the common and specific predictions with a bias term\n",
    "        final_audio_pred = audio_common_pred + audio_specific_pred if audio_common_pred is not None and audio_specific_pred is not None else None\n",
    "        final_visual_pred = visual_common_pred + visual_specific_pred if visual_common_pred is not None and visual_specific_pred is not None else None\n",
    "        \n",
    "        # If both modalities are present, combine the final predictions\n",
    "        combined_pred = None\n",
    "        if final_audio_pred is not None and final_visual_pred is not None:\n",
    "            combined_pred = final_audio_pred + final_visual_pred\n",
    "        elif final_audio_pred is not None:\n",
    "            combined_pred = final_audio_pred\n",
    "        elif final_visual_pred is not None:\n",
    "            combined_pred = final_visual_pred\n",
    "        \n",
    "        return combined_pred, audio_common_pred, audio_specific_pred, visual_common_pred, visual_specific_pred, common_audio_features, common_visual_features, specific_audio_features, specific_visual_features\n",
    "\n",
    "    def compute_loss(self, audio_input, visual_input, labels, alpha1=1.0, alpha2=1.0, alpha3=1.0):\n",
    "        # Forward pass\n",
    "        combined_pred, audio_common_pred, audio_specific_pred, visual_common_pred, visual_specific_pred, common_audio_features, common_visual_features, specific_audio_features, specific_visual_features = self(audio_input, visual_input)\n",
    "\n",
    "        # Initialize losses to zero\n",
    "        similarity_loss = 0.0\n",
    "        auxiliary_loss = 0.0\n",
    "        difference_loss = 0.0\n",
    "\n",
    "        # 1) Similarity Loss (F_sim_k)\n",
    "        if common_audio_features is not None and common_visual_features is not None:\n",
    "            kl_loss_audio = self.compute_kl_divergence(common_audio_features, common_visual_features)\n",
    "            similarity_loss = kl_loss_audio / 2  # Normalized by the number of modalities\n",
    "\n",
    "        # 2) Auxiliary Classification Loss (F_cls_k)\n",
    "        if common_audio_features is not None:\n",
    "            auxiliary_loss += self.compute_auxiliary_classification_loss(common_audio_features, labels)\n",
    "        if common_visual_features is not None:\n",
    "            auxiliary_loss += self.compute_auxiliary_classification_loss(common_visual_features, labels)\n",
    "\n",
    "        # 3) Difference Loss (F_dif_k)\n",
    "        if common_audio_features is not None and specific_audio_features is not None:\n",
    "            difference_loss += self.compute_difference_loss(common_audio_features, specific_audio_features)\n",
    "        if common_visual_features is not None and specific_visual_features is not None:\n",
    "            difference_loss += self.compute_difference_loss(common_visual_features, specific_visual_features)\n",
    "\n",
    "        # 4) Total Loss (F_dec_k)\n",
    "        total_loss = alpha1 * similarity_loss + alpha2 * auxiliary_loss + alpha3 * difference_loss\n",
    "        return total_loss, similarity_loss, auxiliary_loss, difference_loss\n",
    "\n",
    "    def compute_kl_divergence(self, common_audio_features, common_visual_features):\n",
    "        # Apply softmax to features and compute KL divergence\n",
    "        softmax_audio = F.softmax(common_audio_features, dim=-1)\n",
    "        softmax_visual = F.softmax(common_visual_features, dim=-1)\n",
    "        kl_divergence = F.kl_div(softmax_audio.log(), softmax_visual, reduction='batchmean')\n",
    "        return kl_divergence\n",
    "\n",
    "    def compute_auxiliary_classification_loss(self, common_features, labels):\n",
    "        # Cross-entropy loss for auxiliary classification\n",
    "        return F.cross_entropy(self.common_classifier(common_features), labels)\n",
    "\n",
    "    def compute_difference_loss(self, common_features, specific_features):\n",
    "        # Orthogonality loss to ensure modality-common and modality-specific features are distinct\n",
    "        return torch.norm(torch.matmul(common_features.T, specific_features), p='fro')**2\n",
    "\n",
    "\n",
    "# Initialize Model Parameters\n",
    "input_size_audio = 256  # Example size, modify according to your data\n",
    "input_size_visual = 256  # Example size, modify according to your data\n",
    "hidden_size = 128  # Hidden size of the network\n",
    "output_size = 6  # Number of output classes\n",
    "\n",
    "# Initialize the MultiModalNetwork model\n",
    "model = MultiModalNetwork(input_size_audio=input_size_audio, input_size_visual=input_size_visual, hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "# Example input data\n",
    "audio_input = torch.randn(10, input_size_audio)  # Batch of 10 audio samples\n",
    "visual_input = torch.randn(10, input_size_visual)  # Batch of 10 visual samples\n",
    "labels = torch.randint(0, output_size, (10,))  # Random labels for the batch\n",
    "\n",
    "# Compute the total loss with both modalities present\n",
    "total_loss, similarity_loss, auxiliary_loss, difference_loss = model.compute_loss(audio_input, visual_input, labels)\n",
    "\n",
    "print(\"Total Loss:\", total_loss)\n",
    "print(\"Similarity Loss:\", similarity_loss)\n",
    "print(\"Auxiliary Classification Loss:\", auxiliary_loss)\n",
    "print(\"Difference Loss:\", difference_loss)\n",
    "\n",
    "# Compute the total loss with only the audio modality\n",
    "total_loss_audio, similarity_loss_audio, auxiliary_loss_audio, difference_loss_audio = model.compute_loss(audio_input, None, labels)\n",
    "\n",
    "print(\"\\nWhen only audio is present:\")\n",
    "print(\"Total Loss:\", total_loss_audio)\n",
    "print(\"Similarity Loss:\", similarity_loss_audio)\n",
    "print(\"Auxiliary Classification Loss:\", auxiliary_loss_audio)\n",
    "print(\"Difference Loss:\", difference_loss_audio)\n",
    "\n",
    "# Compute the total loss with only the visual modality\n",
    "total_loss_visual, similarity_loss_visual, auxiliary_loss_visual, difference_loss_visual = model.compute_loss(None, visual_input, labels)\n",
    "\n",
    "print(\"\\nWhen only visual is present:\")\n",
    "print(\"Total Loss:\", total_loss_visual)\n",
    "print(\"Similarity Loss:\", similarity_loss_visual)\n",
    "print(\"Auxiliary Classification Loss:\", auxiliary_loss_visual)\n",
    "print(\"Difference Loss:\", difference_loss_visual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4884a566-4491-4fd7-b174-67a14ffa0b97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
