{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f8091cf-fb19-463b-ae9f-68558c286294",
   "metadata": {},
   "source": [
    "# Code starts here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a250ba4c-62a6-4968-8b1c-9be58140dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transf\n",
    "from data_feed import DataFeed, DataFeed_image_pos\n",
    "from build_net import resnet50, NN_beam_pred, MultinomialLogisticRegression\n",
    "\n",
    "# Image Feature Extractor\n",
    "class ImageFeatureExtractor(nn.Module):\n",
    "    def __init__(self, output_dim=128):\n",
    "        super(ImageFeatureExtractor, self).__init__()\n",
    "        base_model = resnet50(pretrained=True, num_classes=64)\n",
    "        #base_model.fc = nn.Identity()  # Remove classification layer\n",
    "        self.feature_extractor = base_model\n",
    "        self.fc = nn.Linear(128, output_dim)  # Project to desired output dimension\n",
    "        #self.bn = nn.BatchNorm1d(output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.fc(x)\n",
    "        #x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "# Position Feature Extractor\n",
    "class PosFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim=4, output_dim=128):\n",
    "        super(PosFeatureExtractor, self).__init__()\n",
    "        self.feature_extractor = NN_beam_pred(num_features=input_dim, num_output=output_dim)\n",
    "        #self.bn = nn.BatchNorm1d(output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        #x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d42fcc2-f34f-45b2-875a-02f8e865d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalNetwork(nn.Module):\n",
    "    def __init__(self, input_size_audio=None, input_size_visual=None, hidden_size=256, output_size=64, z_dim=100):\n",
    "        super(MultiModalNetwork, self).__init__()\n",
    "        \n",
    "        self.has_audio = input_size_audio is not None\n",
    "        self.has_visual = input_size_visual is not None\n",
    "        self.z_dim = z_dim  # Dimensionality of random noise for generator\n",
    "        \n",
    "        if self.has_audio:\n",
    "            # Audio feature extractor\n",
    "            self.audio_feature_extractor = PosFeatureExtractor(output_dim=hidden_size)\n",
    "            \n",
    "            # Common and Specific classifiers for audio\n",
    "            self.audio_common_classifier = nn.Linear(hidden_size // 2, output_size)\n",
    "            self.audio_specific_classifier = nn.Linear(hidden_size // 2, output_size)\n",
    "        \n",
    "        if self.has_visual:\n",
    "            # Visual feature extractor\n",
    "            self.visual_feature_extractor = ImageFeatureExtractor(output_dim=hidden_size)\n",
    "\n",
    "            # Common and Specific classifiers for visual\n",
    "            self.visual_common_classifier = nn.Linear(hidden_size // 2, output_size)\n",
    "            self.visual_specific_classifier = nn.Linear(hidden_size // 2, output_size)\n",
    "        \n",
    "        # Common classifier shared by both modalities\n",
    "        self.common_classifier = nn.Linear(hidden_size // 2, output_size)  # Common features\n",
    "\n",
    "        # Generator Network for learning modality-common features\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(z_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),  # Output common modality features\n",
    "        )\n",
    "\n",
    "    def forward(self, audio_input=None, visual_input=None, z=None):\n",
    "        audio_features = self.audio_feature_extractor(audio_input) if self.has_audio and audio_input is not None else None\n",
    "        visual_features = self.visual_feature_extractor(visual_input) if self.has_visual and visual_input is not None else None\n",
    "\n",
    "        common_audio_features = None\n",
    "        common_visual_features = None\n",
    "        specific_audio_features = None\n",
    "        specific_visual_features = None\n",
    "        \n",
    "        if self.has_audio and audio_features is not None:\n",
    "            # Split audio features into common and specific parts\n",
    "            common_audio_features = audio_features[:, :audio_features.size(1) // 2]\n",
    "            specific_audio_features = audio_features[:, audio_features.size(1) // 2:]\n",
    "        \n",
    "        if self.has_visual and visual_features is not None:\n",
    "            # Split visual features into common and specific parts\n",
    "            common_visual_features = visual_features[:, :visual_features.size(1) // 2]\n",
    "            specific_visual_features = visual_features[:, visual_features.size(1) // 2:]\n",
    "\n",
    "        # Generate modality-common features if z is provided (for knowledge distillation)\n",
    "        generated_common_features = None\n",
    "        generated_common_pred = None\n",
    "\n",
    "        if z is not None:\n",
    "            generated_common_features = self.generator(z)  # Generated audio common features\n",
    "            generated_common_pred = self.common_classifier(generated_common_features)  # Generated audio common features\n",
    "\n",
    "        # Process each modality's common features with their classifiers\n",
    "        final_audio_pred = None\n",
    "        final_visual_pred = None\n",
    "        mean_common_features = 0\n",
    "        modality = 0\n",
    "\n",
    "        if common_audio_features is not None:\n",
    "            modality += 1\n",
    "            mean_common_features += common_audio_features if mean_common_features is not None else 0\n",
    "            common_audio_pred = self.audio_common_classifier(common_audio_features)\n",
    "            specific_audio_pred = self.audio_specific_classifier(specific_audio_features)\n",
    "            final_audio_pred = common_audio_pred + specific_audio_pred  # Combining both predictions\n",
    "\n",
    "        if common_visual_features is not None:\n",
    "            modality += 1\n",
    "            mean_common_features += common_visual_features if mean_common_features is not None else 0\n",
    "            common_visual_pred = self.visual_common_classifier(common_visual_features)\n",
    "            specific_visual_pred = self.visual_specific_classifier(specific_visual_features)\n",
    "            final_visual_pred = common_visual_pred + specific_visual_pred  # Combining both predictions\n",
    "\n",
    "        # Normalize mean_common_features by the number of contributing modalities\n",
    "        if modality > 0:\n",
    "            mean_common_features = mean_common_features / modality  \n",
    "\n",
    "        # Compute final prediction by averaging predictions of all classifiers\n",
    "        if final_audio_pred is not None and final_visual_pred is not None:\n",
    "            final_prediction = (final_audio_pred + final_visual_pred) \n",
    "        elif final_audio_pred is not None:\n",
    "            final_prediction = final_audio_pred\n",
    "        elif final_visual_pred is not None:\n",
    "            final_prediction = final_visual_pred\n",
    "        else:\n",
    "            final_prediction = None  # No valid predictions\n",
    "\n",
    "        \n",
    "\n",
    "        return final_prediction, (final_audio_pred, final_visual_pred, common_audio_features, common_visual_features, specific_audio_features, specific_visual_features, generated_common_features, generated_common_pred, labels, mean_common_features)\n",
    "    def compute_loss(self, audio_input, visual_input, labels, z, alpha1=1.0, alpha2=1.0, alpha_gen=1.0, beta=1.0, alpha_kd=1.0):\n",
    "        # Forward pass\n",
    "        final_prediction, (final_audio_pred, final_visual_pred, common_audio_features, common_visual_features, specific_audio_features, specific_visual_features, generated_common_features, generated_common_pred, labels, mean_common_features) = self(audio_input, visual_input, z)\n",
    "\n",
    "        # Initialize losses to zero\n",
    "        similarity_loss = 0.0\n",
    "        auxiliary_loss = 0.0\n",
    "        difference_loss = 0.0\n",
    "        generation_loss = 0.0\n",
    "        kd_loss = 0.0  # Knowledge Distillation Loss\n",
    "\n",
    "        # 1) Knowledge Distillation Loss (using the local generator)\n",
    "        if common_audio_features is not None:\n",
    "            kd_loss += self.compute_knowledge_distillation_loss(common_audio_features, z)\n",
    "        if common_visual_features is not None:\n",
    "            kd_loss += self.compute_knowledge_distillation_loss(common_visual_features, z)\n",
    "\n",
    "        # 2) Similarity Loss (F_sim_k)\n",
    "        if common_audio_features is not None and common_visual_features is not None:\n",
    "            kl_loss_audio = self.compute_kl_divergence(common_audio_features, common_visual_features)\n",
    "            similarity_loss = kl_loss_audio / 2  # Normalized by the number of modalities\n",
    "\n",
    "        # 3) Auxiliary Classification Loss (F_cls_k)\n",
    "        if common_audio_features is not None:\n",
    "            auxiliary_loss += self.compute_auxiliary_classification_loss(common_audio_features, labels)\n",
    "        if common_visual_features is not None:\n",
    "            auxiliary_loss += self.compute_auxiliary_classification_loss(common_visual_features, labels)\n",
    "\n",
    "        # 4) Difference Loss (F_dif_k) - Orthogonality between common and specific features\n",
    "        if common_audio_features is not None and specific_audio_features is not None:\n",
    "            difference_loss += self.compute_difference_loss(common_audio_features, specific_audio_features)\n",
    "        if common_visual_features is not None and specific_visual_features is not None:\n",
    "            difference_loss += self.compute_difference_loss(common_visual_features, specific_visual_features)\n",
    "\n",
    "        # 5) Generation Loss (F_gen_k)\n",
    "        if generated_common_features is not None:\n",
    "            generation_loss += self.compute_generation_loss(generated_common_features, generated_common_pred, mean_common_features, labels, beta) \n",
    "\n",
    "        # 6) Total Loss (F_dec_k)\n",
    "        total_loss = alpha1 * similarity_loss + alpha2 * difference_loss + auxiliary_loss + alpha_gen * generation_loss + alpha_kd * kd_loss\n",
    "        return total_loss, similarity_loss, auxiliary_loss, difference_loss, generation_loss, kd_loss\n",
    "\n",
    "    def compute_knowledge_distillation_loss(self, common_features, z):\n",
    "        # Generate modality-common features using the local generator (input noise z)\n",
    "        generated_features = self.generator(z)  # Generate modality-common features from noise\n",
    "    \n",
    "        # Pass both real common features and generated features through the common classifier\n",
    "        common_features_pred = self.common_classifier(common_features)\n",
    "        generated_features_pred = self.common_classifier(generated_features)\n",
    "    \n",
    "        # Apply softmax to both the predicted features\n",
    "        softmax_common_features = F.softmax(common_features_pred, dim=-1)\n",
    "        softmax_generated_features = F.softmax(generated_features_pred, dim=-1)\n",
    "    \n",
    "        # Compute KL divergence between the softmax outputs of the real and generated features\n",
    "        kd_loss = F.kl_div(softmax_common_features.log(), softmax_generated_features, reduction='batchmean')\n",
    "    \n",
    "        return kd_loss\n",
    "\n",
    "    def compute_kl_divergence(self, common_audio_features, common_visual_features):\n",
    "        # Apply softmax to features and compute KL divergence\n",
    "        softmax_audio = F.softmax(common_audio_features, dim=-1)\n",
    "        softmax_visual = F.softmax(common_visual_features, dim=-1)\n",
    "        kl_divergence = F.kl_div(softmax_audio.log(), softmax_visual, reduction='batchmean')\n",
    "        return kl_divergence\n",
    "\n",
    "    def compute_auxiliary_classification_loss(self, common_features, labels):\n",
    "        # Cross-entropy loss for auxiliary classification\n",
    "        return F.cross_entropy(self.common_classifier(common_features), labels)\n",
    "\n",
    "    def compute_difference_loss(self, common_features, specific_features):\n",
    "        # Orthogonality loss to ensure modality-common and modality-specific features are distinct\n",
    "        return torch.norm(torch.matmul(common_features.T, specific_features), p='fro')**2\n",
    "\n",
    "    def compute_generation_loss(self, generated_common_features, generated_common_pred, mean_common_features, labels, beta):\n",
    "        # Mean squared error loss to ensure the generated features align with the true features\n",
    "        generated_common_pred = F.softmax(generated_common_pred, dim=-1)\n",
    "        return F.cross_entropy(generated_common_pred, labels) + beta * F.mse_loss(generated_common_features, mean_common_features)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfd61018-1387-426f-ae75-00036e193615",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n"
     ]
    }
   ],
   "source": [
    "# Initialize Model Parameters\n",
    "input_size_audio = 4  # Example size, modify according to your data\n",
    "input_size_visual = 224  # Example size, modify according to your data\n",
    "hidden_size = 256  # Hidden size of the network\n",
    "output_size = 64  # Number of output classes\n",
    "z_dim = 100  # Dimensionality of the generator's input noise\n",
    "\n",
    "# Initialize the MultiModalNetwork model\n",
    "model = MultiModalNetwork(input_size_audio=input_size_audio, input_size_visual=input_size_visual, hidden_size=hidden_size, output_size=output_size, z_dim=z_dim)\n",
    "\n",
    "# Example input data\n",
    "audio_input = torch.randn(10, input_size_audio)  # Batch of 10 audio samples\n",
    "visual_input = torch.randn(10, 3, input_size_visual, input_size_visual)  # Batch of 10 visual samples\n",
    "labels = torch.randint(0, output_size, (10,))  # Random labels for the batch\n",
    "z = torch.randn(10, z_dim)  # Random noise for generator\n",
    "\n",
    "x,_ = model(audio_input=audio_input, visual_input=visual_input, z=z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a48fac37-a679-4cb6-9cc1-27c3aa2e2987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([62, 29, 36, 60, 25, 33, 33, 16, 31,  1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb19823a-28a9-42a5-b795-2d34bf7fe4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss: tensor(32704.4082, grad_fn=<AddBackward0>)\n",
      "Similarity Loss: tensor(0.0440, grad_fn=<DivBackward0>)\n",
      "Auxiliary Classification Loss: tensor(8.3068, grad_fn=<AddBackward0>)\n",
      "Difference Loss: tensor(32691.8125, grad_fn=<AddBackward0>)\n",
      "Generation Loss: tensor(4.2153, grad_fn=<AddBackward0>)\n",
      "Knowledge Distillation Loss: tensor(0.0293, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute the total loss with both modalities present\n",
    "total_loss, similarity_loss, auxiliary_loss, difference_loss, generation_loss, kd_loss = model.compute_loss(audio_input, visual_input, labels, z)\n",
    "\n",
    "print(\"Total Loss:\", total_loss)\n",
    "print(\"Similarity Loss:\", similarity_loss)\n",
    "print(\"Auxiliary Classification Loss:\", auxiliary_loss)\n",
    "print(\"Difference Loss:\", difference_loss)\n",
    "print(\"Generation Loss:\", generation_loss)\n",
    "print(\"Knowledge Distillation Loss:\", kd_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce9ff3d0-cef8-4236-9a59-6f8c085a1fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [03:21<30:12, 201.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 342.80546870231626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [06:42<26:51, 201.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 10.78820011138916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [10:26<24:40, 211.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 9.52267520904541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [14:17<21:55, 219.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 8.88086745262146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [17:37<17:41, 212.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 8.543712978363038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [20:57<13:52, 208.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 8.472490768432618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [24:16<10:15, 205.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 8.4392414188385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [27:43<06:51, 205.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 8.430868740081786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [31:09<03:25, 205.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 8.410905723571778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [34:32<00:00, 207.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 8.397985458374023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 224]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m audio_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, input_size_audio)  \u001b[38;5;66;03m# Single audio sample\u001b[39;00m\n\u001b[0;32m     55\u001b[0m visual_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, input_size_visual)  \u001b[38;5;66;03m# Single visual sample\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m audio_pred, visual_pred \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisual_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio Prediction:\u001b[39m\u001b[38;5;124m\"\u001b[39m, audio_pred)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVisual Prediction:\u001b[39m\u001b[38;5;124m\"\u001b[39m, visual_pred)\n",
      "Cell \u001b[1;32mIn[35], line 50\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, audio_input, visual_input)\u001b[0m\n\u001b[0;32m     48\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# No need to calculate gradients during inference\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     final_pred, (final_audio_pred, final_visual_pred, \u001b[38;5;241m*\u001b[39m_) \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisual_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_audio_pred, final_visual_pred\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[24], line 39\u001b[0m, in \u001b[0;36mMultiModalNetwork.forward\u001b[1;34m(self, audio_input, visual_input, z)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, audio_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, visual_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, z\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     38\u001b[0m     audio_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_feature_extractor(audio_input) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_audio \u001b[38;5;129;01mand\u001b[39;00m audio_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     visual_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual_feature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisual_input\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_visual \u001b[38;5;129;01mand\u001b[39;00m visual_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     common_audio_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     common_visual_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m, in \u001b[0;36mImageFeatureExtractor.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m#x = self.bn(x)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Downloads\\Git_Projects\\Multimodal\\build_net.py:213\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 213\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m    215\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 224]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "# Initialize the optimizer (Adam in this case)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train(model, audio_loader, visual_loader, labels_loader, z_loader, num_epochs=10, alpha1=1.0, alpha2=1.0, alpha_gen=1.0, alpha_kd=1.0):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        running_loss = 0.0\n",
    "        for audio_input, visual_input, labels, z in zip(audio_loader, visual_loader, labels_loader, z_loader):\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "            # Compute the loss\n",
    "            loss_outputs = model.compute_loss(audio_input, visual_input, labels, z, alpha1, alpha2, alpha_gen, alpha_kd)\n",
    "            total_loss, similarity_loss, auxiliary_loss, difference_loss, generation_loss, kd_loss = loss_outputs\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()  # Update the model parameters\n",
    "\n",
    "            running_loss += total_loss.item()\n",
    "\n",
    "        # Print the average loss after each epoch\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(audio_loader)}\")\n",
    "\n",
    "# Initialize Model Parameters\n",
    "input_size_audio = 4  # Example size, modify according to your data\n",
    "input_size_visual = 224  # Example size, modify according to your data\n",
    "hidden_size = 256  # Hidden size of the network\n",
    "output_size = 64  # Number of output classes\n",
    "z_dim = 100  # Dimensionality of the generator's input noise\n",
    "\n",
    "# Simulate 100 batches of random tensors for audio, visual, labels, and z\n",
    "audio_loader = [torch.randn(10, input_size_audio) for _ in range(100)]\n",
    "visual_loader = [torch.randn(10, 3, input_size_visual, input_size_visual) for _ in range(100)]\n",
    "labels_loader = [torch.randint(0, output_size, (10,)) for _ in range(100)]\n",
    "z_loader = [torch.randn(10, z_dim) for _ in range(100)]\n",
    "\n",
    "# Train the model\n",
    "train(model, audio_loader, visual_loader, labels_loader, z_loader, num_epochs=10)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, audio_input, visual_input):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # No need to calculate gradients during inference\n",
    "        final_pred, (final_audio_pred, final_visual_pred, *_) = model(audio_input, visual_input)\n",
    "        return final_audio_pred, final_visual_pred\n",
    "\n",
    "# Example evaluation\n",
    "audio_input = torch.randn(1, input_size_audio)  # Single audio sample\n",
    "visual_input = torch.randn(1, input_size_visual)  # Single visual sample\n",
    "audio_pred, visual_pred = evaluate(model, audio_input, visual_input)\n",
    "print(\"Audio Prediction:\", audio_pred)\n",
    "print(\"Visual Prediction:\", visual_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b640e93a-557d-430f-9c45-1ece053186fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transf\n",
    "from data_feed import DataFeed, DataFeed_image_pos\n",
    "from build_net import resnet50, NN_beam_pred, MultinomialLogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cbaad3-21e2-4ea2-8ae5-0e033a11f6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# or full reproducibility\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6891313-8761-41b4-974a-ea6a00d78d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Now you can use the `device` variable to move your model and data to the correct device\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa0ef78-25f1-4943-9f96-2dc1dce0b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the saved CSV files\n",
    "output_dir = \"./feature_nonIID/\"\n",
    "\n",
    "# Load one of the CSV files for EDA (e.g., user_0_outputs.csv)\n",
    "df = pd.read_csv(output_dir + \"user_0_pos_height_beam.csv\")\n",
    "\n",
    "# Quick overview of the data\n",
    "print(\"Data Overview:\")\n",
    "print(df.head())\n",
    "print(\"\\nData Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cc216a-d03d-4717-b2e0-d53f773a37e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "########################### Data pre-processing ########################\n",
    "########################################################################\n",
    "no_users = 20\n",
    "batch_size = 64\n",
    "img_resize = transf.Resize((224, 224))\n",
    "img_norm = transf.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "proc_pipe = transf.Compose(\n",
    "    [transf.ToPILImage(),\n",
    "     img_resize,\n",
    "     transf.ToTensor(),\n",
    "     img_norm]\n",
    ")\n",
    "dataset_dir = \"feature_nonIID/\"\n",
    "train_loaders = []\n",
    "test_loaders = []\n",
    "val_loaders = []\n",
    "\n",
    "for user_id in range(no_users):\n",
    "    train_dir = dataset_dir + f'user_{user_id}_pos_height_beam_train.csv'\n",
    "    val_dir = dataset_dir + f'user_{user_id}_pos_height_beam_val.csv'\n",
    "    test_dir = dataset_dir + f'user_{user_id}_pos_height_beam_test.csv'\n",
    "    \n",
    "    train_dataset = DataFeed_image_pos(train_dir, transform=proc_pipe)\n",
    "    val_dataset = DataFeed_image_pos(root_dir=val_dir, transform=proc_pipe)\n",
    "    test_dataset = DataFeed_image_pos(root_dir=test_dir, transform=proc_pipe)\n",
    "    \n",
    "    \n",
    "    train_loaders.append(DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              #num_workers=8,\n",
    "                              shuffle=True))\n",
    "    val_loaders.append(DataLoader(val_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            #num_workers=8,\n",
    "                            shuffle=False))\n",
    "    test_loaders.append(DataLoader(test_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            #num_workers=8,\n",
    "                            shuffle=False))\n",
    "print(\"All loadred are loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a4842a-bbbf-475f-bdbf-100f81e971ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Preperation#\n",
    "all_models = []\n",
    "available_modalities = [\"pos_height\", \"images\"]\n",
    "modality_size = {\"pos_height\": 128, \"images\": 128}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a9572a-e88f-431d-88a5-08aff378f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import random\n",
    "no_users = 20  # Example: Number of users\n",
    "available_modalities = [\"pos_height\", \"images\"]\n",
    "modality_size = {\"pos_height\": 128, \"images\": 128}\n",
    "group_definitions = {\n",
    "    1: [\"pos_height\"],        # Group 1: Only pos_height\n",
    "    2: [\"images\"],            # Group 2: Only images\n",
    "    3: [\"pos_height\", \"images\"]  # Group 3: Both modalities\n",
    "}\n",
    "\n",
    "# Assign each user to a group randomly\n",
    "weights = [0.2, 0.3, 0.5]  # Probabilities for groups 1, 2, and 3\n",
    "\n",
    "# Generate user_groups with weighted random choices\n",
    "user_groups = random.choices([1, 2, 3], weights=weights, k=no_users)\n",
    "\n",
    "# Assign modalities to users based on their group\n",
    "user_modalities = [group_definitions[group] for group in user_groups]\n",
    "\n",
    "# Compute output sizes for each user based on their modalities\n",
    "output_sizes = [sum(modality_size[modality] for modality in user_modality) for user_modality in user_modalities]\n",
    "\n",
    "# Store models (placeholders for actual models)\n",
    "all_models = []\n",
    "\n",
    "# Example output (for verification)\n",
    "print(f\"User Groups: {user_groups[:10]}\")  # Show first 10 users' groups\n",
    "print(f\"User Modalities: {user_modalities[:10]}\")  # Show first 10 users' modalities\n",
    "print(f\"Output Sizes: {output_sizes[:10]}\")  # Show first 10 users' output sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4df557-5b41-4333-b208-bf6435842c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_knopp(matrix, tol=1e-9, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Converts a given matrix to a doubly stochastic matrix using the Sinkhorn-Knopp algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "        matrix (np.ndarray): The input matrix to be transformed.\n",
    "        tol (float): The tolerance for convergence.\n",
    "        max_iter (int): Maximum number of iterations for convergence.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A doubly stochastic matrix.\n",
    "    \"\"\"\n",
    "    matrix = matrix.copy()\n",
    "    for _ in range(max_iter):\n",
    "        # Normalize rows\n",
    "        row_sums = matrix.sum(axis=1, keepdims=True)\n",
    "        matrix /= row_sums\n",
    "\n",
    "        # Normalize columns\n",
    "        col_sums = matrix.sum(axis=0, keepdims=True)\n",
    "        matrix /= col_sums\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.allclose(matrix.sum(axis=1), 1, atol=tol) and np.allclose(matrix.sum(axis=0), 1, atol=tol):\n",
    "            break\n",
    "\n",
    "    return matrix\n",
    "    \n",
    "def create_random_topology(num_users, similarity_matrix, edge_probability=0.3):\n",
    "    \"\"\"\n",
    "    Creates a connected random topology using NetworkX.\n",
    "    Returns the adjacency matrix.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        graph = nx.erdos_renyi_graph(num_users, edge_probability)\n",
    "        adjacency_matrix = nx.to_numpy_array(graph)\n",
    "        new_adj = np.multiply(adjacency_matrix, similarity_matrix)\n",
    "        new_graph = nx.from_numpy_array(new_adj)\n",
    "        if nx.is_connected(new_graph):\n",
    "            break\n",
    "\n",
    "    # Convert graph to adjacency matrix\n",
    "    adjacency_matrix = nx.to_numpy_array(new_graph)\n",
    "    return adjacency_matrix\n",
    "\n",
    "def prepare_mixing_matrices(adjacency_matrix, similarity_matrices):\n",
    "    \"\"\"\n",
    "    Computes a mixing matrix for each modality by multiplying the adjacency matrix \n",
    "    with the similarity matrix for that modality.\n",
    "    Returns a dictionary of mixing matrices.\n",
    "    \"\"\"\n",
    "    adjacency_matrices = {}\n",
    "    mixing_matrices = {}\n",
    "    for modality, similarity_matrix in similarity_matrices.items():\n",
    "        # Element-wise multiplication of adjacency and similarity matrices\n",
    "        combined_matrix = adjacency_matrix * similarity_matrix\n",
    "        adjacency_matrices[modality] = combined_matrix\n",
    "        \n",
    "        # Normalize to create a doubly matrix\n",
    "        mixing_matrix = sinkhorn_knopp(combined_matrix)\n",
    "        \n",
    "        \n",
    "        mixing_matrices[modality] = mixing_matrix\n",
    "    \n",
    "    return mixing_matrices, adjacency_matrices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e735154-ee14-48d4-b967-90c61895a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random connected topology\n",
    "#adjacency_matrix = create_random_topology(no_users, edge_probability=0.3)\n",
    "# Initialize adjacency matrix\n",
    "similarity_matrix = np.zeros((no_users, no_users), dtype=int)\n",
    "\n",
    "# Construct the adjacency matrix\n",
    "for i in range(no_users):\n",
    "    for j in range(no_users):\n",
    "        if i != j:  # No self-loops\n",
    "            # Check if users i and j share any modalities\n",
    "            if set(user_modalities[i]) & set(user_modalities[j]):\n",
    "                similarity_matrix[i, j] = 1\n",
    "\n",
    "# Display the adjacency matrix\n",
    "print(\"Adjacency Matrix:\")\n",
    "print(similarity_matrix)\n",
    "\n",
    "# Prepare mixing matrices for each modality\n",
    "#mixing_matrices, adjacency_matrices = prepare_mixing_matrices(adjacency_matrix, similarity_matrices)\n",
    "adjacency_matrix = create_random_topology(20, similarity_matrix, edge_probability=0.3)\n",
    "print(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a0e1f-5445-4181-bce0-4bc282edde8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the graph\n",
    "# Define colors for the groups\n",
    "group_colors = {1: 'red', 2: 'green', 3: 'blue'}\n",
    "node_colors = [group_colors[group] for group in user_groups]\n",
    "G = nx.from_numpy_array(similarity_matrix)\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, edge_color='gray', node_size=1000, node_color=node_colors, font_size=20, font_color='black')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df54d0c1-b6b7-409c-89fe-789f41818e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the graph\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "G = nx.from_numpy_array(adjacency_matrix)\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, edge_color='gray', node_size=1000, node_color=node_colors, font_size=20, font_color='black')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "print(user_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56fdf93-74f1-4eab-9bba-2aadb6cc097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity matrices\n",
    "adj_per_modality = {}\n",
    "for modality in available_modalities:\n",
    "    adj = np.zeros((no_users, no_users))\n",
    "    for node in range(no_users):\n",
    "        for neighbor in G.neighbors(node):\n",
    "            if modality in user_modalities[neighbor] and modality in user_modalities[node]:\n",
    "                adj[node, neighbor] = 1.    \n",
    "    adj_per_modality[modality] = adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb316e-d5ab-4c2f-bd38-6732228b7e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_modality = nx.from_numpy_array(adj_per_modality[\"images\"])\n",
    "pos = nx.spring_layout(G_modality)\n",
    "nx.draw(G_modality, pos, with_labels=True, edge_color='gray', node_size=1000, node_color=node_colors, font_size=20, font_color='black')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317b907-f7ae-42c0-8921-f31b2cee9f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_mixing_matrix(Adj, method=\"metropolis\"):\n",
    "    n = Adj.shape[0]\n",
    "    W = np.zeros((n, n))  # Initialize weight matrix\n",
    "\n",
    "    for i in range(n):\n",
    "        degree_i = np.sum(Adj[i, :])\n",
    "\n",
    "        for j in range(n):\n",
    "            if Adj[i, j] == 1.0:\n",
    "                degree_j = np.sum(Adj[j, :])\n",
    "    \n",
    "                if method == \"metropolis\":\n",
    "                    W[i, j] = 1 / (max(degree_i, degree_j) + 1)\n",
    "                elif method == \"uniform\":\n",
    "                    W[i, j] = 1 / degree_i\n",
    "\n",
    "        # Diagonal weight\n",
    "        W[i, i] = 1 - np.sum(W[i, :])\n",
    "\n",
    "    return W\n",
    "\n",
    "mixing_matrices = {}\n",
    "for modality in available_modalities:\n",
    "    mixing_matrices[modality] = construct_mixing_matrix(adj_per_modality[modality], method=\"metropolis\")\n",
    "    print(np.sum(mixing_matrices[modality], 0))\n",
    "    print(np.sum(mixing_matrices[modality], 1))\n",
    "    lamb = np.linalg.eigvals(mixing_matrices[modality])\n",
    "    lamb.sort()\n",
    "    print(lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd6829f-6252-40e3-97d0-ad0ea2d4882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_modality = nx.from_numpy_array(adj_per_modality[\"pos_height\"])\n",
    "pos = nx.spring_layout(G_modality)\n",
    "largest_cc = max(nx.connected_components(G_modality), key=len)\n",
    "\n",
    "# Convert to sorted list of indices\n",
    "connected_nodes = sorted(largest_cc)\n",
    "\n",
    "# Extract the submatrix corresponding to the connected subgraph\n",
    "W_reduced = mixing_matrices[\"pos_height\"][np.ix_(connected_nodes, connected_nodes)]\n",
    "lamb = np.linalg.eigvals(W_reduced)\n",
    "lamb.sort()\n",
    "print(lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79709be2-ee33-42c7-9165-2e7171eaf88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Feature Extractor\n",
    "class ImageFeatureExtractor(nn.Module):\n",
    "    def __init__(self, output_dim=128):\n",
    "        super(ImageFeatureExtractor, self).__init__()\n",
    "        base_model = resnet50(pretrained=True, num_classes=64)\n",
    "        #base_model.fc = nn.Identity()  # Remove classification layer\n",
    "        self.feature_extractor = base_model\n",
    "        #self.fc = nn.Linear(128, output_dim)  # Project to desired output dimension\n",
    "        self.bn = nn.BatchNorm1d(output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        #x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "# Position Feature Extractor\n",
    "class PosFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim=4, output_dim=128):\n",
    "        super(PosFeatureExtractor, self).__init__()\n",
    "        self.feature_extractor = NN_beam_pred(num_features=input_dim, num_output=output_dim)\n",
    "        self.bn = nn.BatchNorm1d(output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "# Classification Head\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=64):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Main Model with Named Sub-Networks\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, use_image=True, use_pos=True, feature_dim=128, num_classes=64):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        # Store sub-networks in a dictionary\n",
    "        self.sub_networks = nn.ModuleDict()\n",
    "\n",
    "        if use_image:\n",
    "            self.sub_networks[\"images\"] = ImageFeatureExtractor(output_dim=feature_dim)\n",
    "        if use_pos:\n",
    "            self.sub_networks[\"pos_height\"] = PosFeatureExtractor(output_dim=feature_dim)\n",
    "\n",
    "        # Determine input size for classification head\n",
    "        input_dim = (feature_dim if use_image else 0) + (feature_dim if use_pos else 0)\n",
    "        self.classifier = ClassificationHead(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, images=None, pos_height=None):\n",
    "        features = []\n",
    "\n",
    "        if \"images\" in self.sub_networks and images is not None:\n",
    "            features.append(self.sub_networks[\"images\"](images))\n",
    "\n",
    "        if \"pos_height\" in self.sub_networks and pos_height is not None:\n",
    "            features.append(self.sub_networks[\"pos_height\"](pos_height))\n",
    "\n",
    "        if not features:\n",
    "            raise ValueError(\"At least one modality (image or pos) must be used\")\n",
    "\n",
    "        x = torch.cat(features, dim=1) if len(features) > 1 else features[0]\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecaef50-beed-43b7-89b7-186b1ac18658",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(use_image=True, use_pos=True)\n",
    "\n",
    "# Extract the image feature extractor using its name\n",
    "image_extractor = model.sub_networks[\"pos_height\"]\n",
    "\n",
    "classifier_head = model.classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441abedd-5799-4950-950b-f23acae1f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "optimizers = []\n",
    "all_models = []\n",
    "classifier_optimizers = []\n",
    "for user_id in range(no_users):\n",
    "    if \"images\" in user_modalities[user_id] and \"pos_height\" in user_modalities[user_id]:\n",
    "        user_model = Classifier(use_image=True, use_pos=True).to(device)\n",
    "    elif \"pos_height\" in user_modalities[user_id]:\n",
    "        user_model = Classifier(use_image=False, use_pos=True).to(device)\n",
    "    elif \"images\" in user_modalities[user_id]:\n",
    "        user_model = Classifier(use_image=True, use_pos=False).to(device)\n",
    "    local_optimizer = optim.Adam(user_model.parameters(), lr=lr)\n",
    "    class_optim = optim.Adam(user_model.classifier.parameters(), lr=lr)\n",
    "\n",
    "    all_models.append(user_model)\n",
    "    optimizers.append(local_optimizer)\n",
    "    classifier_optimizers.append(class_optim)\n",
    "base_models = Classifier(use_image=True, use_pos=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0b559-d49b-4d97-be81-6218622f33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Decentralized aggregation function\n",
    "def per_modelaity_decentralized_aggregation(user_models, mixing_matrices, available_modalities, user_modalities, base_models):\n",
    "    num_users = len(user_models)\n",
    "    with torch.no_grad():\n",
    "        for modality in available_modalities:\n",
    "            # Get the mixing matrix for the current modality\n",
    "            mixing_matrix = mixing_matrices[modality]\n",
    "            \n",
    "            # Convert user model parameters to vectors for aggregation\n",
    "            aggregated_models = []\n",
    "            aggregated_updates = []\n",
    "            for user_id, user_model in enumerate(user_models):\n",
    "                if modality in user_modalities[user_id]:\n",
    "                    aggregated_models.append(torch.nn.utils.parameters_to_vector(user_model.sub_networks[modality].parameters()))\n",
    "                    aggregated_updates.append(torch.zeros_like(aggregated_models[-1]))\n",
    "                else:\n",
    "                    aggregated_models.append(0)\n",
    "                    aggregated_updates.append(0)\n",
    "            \n",
    "            \n",
    "            # Perform model aggregation based on the mixing matrix for this modality\n",
    "            for i in range(num_users):\n",
    "                for j in range(num_users):\n",
    "                    if mixing_matrix[i, j] > 0:\n",
    "                        aggregated_updates[i] += mixing_matrix[i, j] * aggregated_models[j]\n",
    "            \n",
    "            # Update user models with aggregated parameters for the current modality\n",
    "            for user_id in range(num_users):\n",
    "                if modality in user_modalities[user_id]:\n",
    "                    torch.nn.utils.vector_to_parameters(aggregated_updates[user_id], user_models[user_id].sub_networks[modality].parameters())\n",
    "\n",
    "#per_modelaity_decentralized_aggregation(all_models, mixing_matrices, available_modalities, user_modalities, base_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd3ed87-b01b-4f8c-9886-5526789eba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_local_model(local_modalities, model, train_loader, criterion, optimizer, epochs, device):\n",
    "    \"\"\"\n",
    "    Trains a local multi-modal model.\n",
    "\n",
    "    Args:\n",
    "        local_modalities (list): Modalities to use (e.g., ['image', 'pos']).\n",
    "        model (Classifier): Multi-modal classification model.\n",
    "        train_loader (DataLoader): Training data loader.\n",
    "        criterion (nn.CrossEntropyLoss): Loss function.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer.\n",
    "        epochs (int): Number of training epochs.\n",
    "        device (torch.device): Device (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Minimum training loss, maximum training accuracy.\n",
    "    \"\"\"\n",
    "    # Unfreeze the layers\n",
    "    # freezing first layers \n",
    "    for mod in local_modalities:\n",
    "        for param in model.sub_networks[mod].parameters():\n",
    "            param.requires_grad = True  # Freezes the feature extractor\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    training_losses = []\n",
    "    training_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "\n",
    "            # Prepare input data for selected modalities\n",
    "            modality_inputs = {mod: inputs[mod].to(device) for mod in local_modalities}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(**modality_inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(model, outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update metrics\n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        # Compute loss and accuracy\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "        training_losses.append(avg_loss)\n",
    "        training_accuracies.append(accuracy)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return min(training_losses), max(training_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327c51a-a71f-4724-b896-29e02967db31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_user_models(user_id, model, val_loader, criterion, local_modalities, device):\n",
    "    \"\"\"\n",
    "    Validates a trained multi-modal model.\n",
    "\n",
    "    Args:\n",
    "        user_id (int): User identifier.\n",
    "        model (Classifier): Multi-modal classification model.\n",
    "        val_loader (DataLoader): Validation data loader.\n",
    "        criterion (nn.CrossEntropyLoss): Loss function.\n",
    "        local_modalities (list): Modalities to use (e.g., ['image', 'pos']).\n",
    "        device (torch.device): Device (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        dict: Validation loss and accuracy.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels = batch\n",
    "\n",
    "            # Prepare input data for selected modalities\n",
    "            modality_inputs = {mod: inputs[mod].to(device) for mod in local_modalities}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(**modality_inputs)\n",
    "            loss = criterion(model, outputs, labels)\n",
    "\n",
    "            # Accumulate metrics\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Compute loss and accuracy\n",
    "    avg_loss = total_loss / total_samples if total_samples > 0 else 0.0\n",
    "    accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "    print(f\"User {user_id + 1} - Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return {\"loss\": avg_loss, \"accuracy\": accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08ecb69-5a71-4045-9a1e-3375d81fee4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
