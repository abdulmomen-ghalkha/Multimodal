{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f8091cf-fb19-463b-ae9f-68558c286294",
   "metadata": {},
   "source": [
    "# Code starts here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a250ba4c-62a6-4968-8b1c-9be58140dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transf\n",
    "from data_feed import DataFeed, DataFeed_image_pos\n",
    "from build_net import resnet50, NN_beam_pred, MultinomialLogisticRegression, ImageFeatureExtractor, PosFeatureExtractor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5821a84-6188-43b6-82ff-1df9692d6947",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalNetwork(nn.Module):\n",
    "    def __init__(self, input_size_audio=None, input_size_visual=None, hidden_size=256, output_size=64, z_dim=100):\n",
    "        super(MultiModalNetwork, self).__init__()\n",
    "        \n",
    "        self.has_audio = input_size_audio is not None\n",
    "        self.has_visual = input_size_visual is not None\n",
    "        self.z_dim = z_dim  # Dimensionality of random noise for generator\n",
    "        \n",
    "        if self.has_audio:\n",
    "            # Audio feature extractor\n",
    "            self.audio_feature_extractor = PosFeatureExtractor(output_dim=hidden_size)\n",
    "            \n",
    "            # Common and Specific classifiers for audio\n",
    "            #self.audio_common_classifier = nn.Linear(hidden_size // 2, output_size)\n",
    "            self.audio_specific_classifier = nn.Linear(hidden_size // 2, output_size)\n",
    "        \n",
    "        if self.has_visual:\n",
    "            # Visual feature extractor\n",
    "            self.visual_feature_extractor = ImageFeatureExtractor(output_dim=hidden_size)\n",
    "\n",
    "            # Common and Specific classifiers for visual\n",
    "            #self.visual_common_classifier = nn.Linear(hidden_size // 2, output_size)\n",
    "            self.visual_specific_classifier = nn.Linear(hidden_size // 2, output_size)\n",
    "        \n",
    "        # Common classifier shared by both modalities\n",
    "        self.common_classifier = nn.Linear(hidden_size // 2, output_size)  # Common features\n",
    "\n",
    "        # Generator Network for learning modality-common features\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(z_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),  # Output common modality features\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, audio_input=None, visual_input=None, z=None):\n",
    "        audio_features = self.audio_feature_extractor(audio_input) if self.has_audio and audio_input is not None else None\n",
    "        visual_features = self.visual_feature_extractor(visual_input) if self.has_visual and visual_input is not None else None\n",
    "\n",
    "        common_audio_features = None\n",
    "        common_visual_features = None\n",
    "        specific_audio_features = None\n",
    "        specific_visual_features = None\n",
    "        \n",
    "        if self.has_audio and audio_features is not None:\n",
    "            # Split audio features into common and specific parts\n",
    "            common_audio_features = audio_features[:, :audio_features.size(1) // 2]\n",
    "            specific_audio_features = audio_features[:, audio_features.size(1) // 2:]\n",
    "        \n",
    "        if self.has_visual and visual_features is not None:\n",
    "            # Split visual features into common and specific parts\n",
    "            common_visual_features = visual_features[:, :visual_features.size(1) // 2]\n",
    "            specific_visual_features = visual_features[:, visual_features.size(1) // 2:]\n",
    "\n",
    "        # Generate modality-common features if z is provided (for knowledge distillation)\n",
    "        generated_common_features = None\n",
    "        generated_common_pred = None\n",
    "\n",
    "        if z is not None:\n",
    "            generated_common_features = self.generator(z)  # Generated audio common features\n",
    "            generated_common_pred = self.common_classifier(generated_common_features)  # Generated audio common features\n",
    "\n",
    "        # Process each modality's common features with their classifiers\n",
    "        final_audio_pred = None\n",
    "        final_visual_pred = None\n",
    "        mean_common_features = 0\n",
    "        modality = 0\n",
    "\n",
    "        if common_audio_features is not None:\n",
    "            modality += 1\n",
    "            mean_common_features += common_audio_features if mean_common_features is not None else 0\n",
    "            common_audio_pred = self.common_classifier(common_audio_features)\n",
    "            specific_audio_pred = self.audio_specific_classifier(specific_audio_features)\n",
    "            final_audio_pred = common_audio_pred + specific_audio_pred  # Combining both predictions\n",
    "\n",
    "        if common_visual_features is not None:\n",
    "            modality += 1\n",
    "            mean_common_features += common_visual_features if mean_common_features is not None else 0\n",
    "            common_visual_pred = self.common_classifier(common_visual_features)\n",
    "            specific_visual_pred = self.visual_specific_classifier(specific_visual_features)\n",
    "            final_visual_pred = common_visual_pred + specific_visual_pred  # Combining both predictions\n",
    "\n",
    "        # Normalize mean_common_features by the number of contributing modalities\n",
    "        if modality > 0:\n",
    "            mean_common_features = mean_common_features / modality  \n",
    "\n",
    "        # Compute final prediction by averaging predictions of all classifiers\n",
    "        if final_audio_pred is not None and final_visual_pred is not None:\n",
    "            final_prediction = (final_audio_pred + final_visual_pred) \n",
    "        elif final_audio_pred is not None:\n",
    "            final_prediction = final_audio_pred\n",
    "        elif final_visual_pred is not None:\n",
    "            final_prediction = final_visual_pred\n",
    "        else:\n",
    "            final_prediction = None  # No valid predictions\n",
    "\n",
    "        \n",
    "\n",
    "        return final_prediction, (final_audio_pred, final_visual_pred, common_audio_features, common_visual_features, specific_audio_features, specific_visual_features, generated_common_features, generated_common_pred, mean_common_features)\n",
    "    def compute_loss(self, audio_input, visual_input, labels, z, alpha0=1.0, alpha1=1.0, alpha2=1.0, alpha3=1.0, alpha_gen=1.0, alpha_kd=1.0):\n",
    "        # Forward pass\n",
    "        final_prediction, (final_audio_pred, final_visual_pred, common_audio_features, common_visual_features, specific_audio_features, specific_visual_features, generated_common_features, generated_common_pred, mean_common_features) = self(audio_input, visual_input, z)\n",
    "\n",
    "        # Initialize losses to zero\n",
    "        similarity_loss = 0.0\n",
    "        auxiliary_loss = 0.0\n",
    "        difference_loss = 0.0\n",
    "        generation_loss = 0.0\n",
    "        kd_loss = 0.0  # Knowledge Distillation Loss\n",
    "        classification_loss = 0.0\n",
    "\n",
    "        # 1) Knowledge Distillation Loss (using the local generator)\n",
    "        if common_audio_features is not None:\n",
    "            kd_loss += self.compute_knowledge_distillation_loss(common_audio_features, z)\n",
    "        if common_visual_features is not None:\n",
    "            kd_loss += self.compute_knowledge_distillation_loss(common_visual_features, z)\n",
    "\n",
    "        # 2) Similarity Loss (F_sim_k)\n",
    "        if common_audio_features is not None and common_visual_features is not None:\n",
    "            kl_loss_audio = self.compute_kl_divergence(common_audio_features, common_visual_features)\n",
    "            similarity_loss = kl_loss_audio / 2  # Normalized by the number of modalities\n",
    "\n",
    "        # 3) Auxiliary Classification Loss (F_cls_k)\n",
    "        if common_audio_features is not None:\n",
    "            auxiliary_loss += self.compute_auxiliary_classification_loss(common_audio_features, labels)\n",
    "        if common_visual_features is not None:\n",
    "            auxiliary_loss += self.compute_auxiliary_classification_loss(common_visual_features, labels)\n",
    "\n",
    "        # 4) Difference Loss (F_dif_k) - Orthogonality between common and specific features\n",
    "        if common_audio_features is not None and specific_audio_features is not None:\n",
    "            difference_loss += self.compute_difference_loss(common_audio_features, specific_audio_features)\n",
    "        if common_visual_features is not None and specific_visual_features is not None:\n",
    "            difference_loss += self.compute_difference_loss(common_visual_features, specific_visual_features)\n",
    "\n",
    "        # 5) Generation Loss (F_gen_k)\n",
    "        if generated_common_features is not None:\n",
    "            generation_loss += self.compute_generation_loss(generated_common_features, generated_common_pred, mean_common_features, labels)\n",
    "\n",
    "        classification_loss += self.compute_classification_loss(final_prediction, labels)\n",
    "        \n",
    "\n",
    "        # 6) Total Loss (F_dec_k)\n",
    "        total_loss = alpha0 * classification_loss + alpha1 * similarity_loss + alpha2 * difference_loss + alpha3 * auxiliary_loss + alpha_gen * generation_loss + alpha_kd * kd_loss\n",
    "        return total_loss, classification_loss, similarity_loss, auxiliary_loss, difference_loss, generation_loss, kd_loss\n",
    "\n",
    "    def compute_knowledge_distillation_loss(self, common_features, z):\n",
    "        # Generate modality-common features using the local generator (input noise z)\n",
    "        generated_features = self.generator(z)  # Generate modality-common features from noise\n",
    "    \n",
    "        # Pass both real common features and generated features through the common classifier\n",
    "        common_features_pred = self.common_classifier(common_features)\n",
    "        generated_features_pred = self.common_classifier(generated_features)\n",
    "    \n",
    "        # Apply softmax to both the predicted features\n",
    "        softmax_common_features = F.softmax(common_features_pred, dim=-1)\n",
    "        softmax_generated_features = F.softmax(generated_features_pred, dim=-1)\n",
    "    \n",
    "        # Compute KL divergence between the softmax outputs of the real and generated features\n",
    "        kd_loss = F.kl_div(softmax_common_features.log(), softmax_generated_features, reduction='batchmean')\n",
    "    \n",
    "        return kd_loss\n",
    "\n",
    "    def compute_kl_divergence(self, common_audio_features, common_visual_features):\n",
    "        # Apply softmax to features and compute KL divergence\n",
    "        softmax_audio = F.softmax(common_audio_features, dim=-1)\n",
    "        softmax_visual = F.softmax(common_visual_features, dim=-1)\n",
    "        kl_divergence = F.kl_div(softmax_audio.log(), softmax_visual, reduction='batchmean')\n",
    "        return kl_divergence\n",
    "\n",
    "    def compute_auxiliary_classification_loss(self, common_features, labels):\n",
    "        # Cross-entropy loss for auxiliary classification\n",
    "        return F.cross_entropy(self.common_classifier(common_features), labels)\n",
    "\n",
    "    def compute_difference_loss(self, common_features, specific_features):\n",
    "        # Orthogonality loss to ensure modality-common and modality-specific features are distinct\n",
    "        return torch.norm(torch.matmul(common_features.T, specific_features), p='fro')**2\n",
    "\n",
    "    def compute_generation_loss(self, generated_common_features, generated_common_pred, mean_common_features, labels):\n",
    "        # Mean squared error loss to ensure the generated features align with the true features\n",
    "        generated_common_pred = F.softmax(generated_common_pred, dim=-1)\n",
    "        beta = 1.0\n",
    "        return F.cross_entropy(generated_common_pred, labels) + beta * F.mse_loss(generated_common_features, mean_common_features)\n",
    "\n",
    "    def compute_classification_loss(self, preds, label):\n",
    "        \n",
    "        return self.criterion(preds, label)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd61018-1387-426f-ae75-00036e193615",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize Model Parameters\n",
    "input_size_audio = 4  # Example size, modify according to your data\n",
    "input_size_visual = 224  # Example size, modify according to your data\n",
    "hidden_size = 256  # Hidden size of the network\n",
    "output_size = 64  # Number of output classes\n",
    "z_dim = 100  # Dimensionality of the generator's input noise\n",
    "\n",
    "# Initialize the MultiModalNetwork model\n",
    "model = MultiModalNetwork(input_size_audio=input_size_audio, input_size_visual=input_size_visual, hidden_size=hidden_size, output_size=output_size, z_dim=z_dim)\n",
    "\n",
    "# Example input data\n",
    "audio_input = torch.randn(10, input_size_audio)  # Batch of 10 audio samples\n",
    "visual_input = torch.randn(10, 3, input_size_visual, input_size_visual)  # Batch of 10 visual samples\n",
    "labels = torch.randint(0, output_size, (10,))  # Random labels for the batch\n",
    "z = torch.randn(10, z_dim)  # Random noise for generator\n",
    "\n",
    "x = model(audio_input=audio_input, visual_input=visual_input, z=z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fac37-a679-4cb6-9cc1-27c3aa2e2987",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb19823a-28a9-42a5-b795-2d34bf7fe4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the total loss with both modalities present\n",
    "alpha = 1e-6\n",
    "num_epochs=1\n",
    "alpha0 = 1e1\n",
    "alpha1=alpha\n",
    "alpha2=alpha\n",
    "alpha3 = alpha\n",
    "alpha_gen=alpha\n",
    "alpha_kd=alpha\n",
    "total_loss, classification_loss, similarity_loss, auxiliary_loss, difference_loss, generation_loss, kd_loss = model.compute_loss(audio_input, visual_input, labels, z, alpha0, alpha1, alpha2, alpha3, alpha_gen, alpha_kd)\n",
    "# Print all the losses\n",
    "print(f\"Total Loss: {total_loss}\")\n",
    "print(f\"Classification Loss: {classification_loss}\")\n",
    "print(f\"Similarity Loss: {similarity_loss}\")\n",
    "print(f\"Auxiliary Loss: {auxiliary_loss}\")\n",
    "print(f\"Difference Loss: {difference_loss}\")\n",
    "print(f\"Generation Loss: {generation_loss}\")\n",
    "print(f\"Knowledge Distillation Loss: {kd_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9ff3d0-cef8-4236-9a59-6f8c085a1fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "# Initialize the optimizer (Adam in this case)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train(model, audio_loader, visual_loader, labels_loader, z_loader, num_epochs=10, alpha0=1.0, alpha1=1.0, alpha2=1.0, alpha_gen=1.0, alpha_kd=1.0):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        running_loss = 0.0\n",
    "        for audio_input, visual_input, labels, z in zip(audio_loader, visual_loader, labels_loader, z_loader):\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "            # Compute the loss\n",
    "            loss_outputs = model.compute_loss(audio_input, visual_input, labels, z, alpha0, alpha1, alpha2, alpha_gen, alpha_kd)\n",
    "            total_loss, class_loss, similarity_loss, auxiliary_loss, difference_loss, generation_loss, kd_loss = loss_outputs\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()  # Update the model parameters\n",
    "\n",
    "            running_loss += total_loss.item()\n",
    "\n",
    "        # Print the average loss after each epoch\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(audio_loader)}\")\n",
    "\n",
    "# Initialize Model Parameters\n",
    "input_size_audio = 4  # Example size, modify according to your data\n",
    "input_size_visual = 224  # Example size, modify according to your data\n",
    "hidden_size = 256  # Hidden size of the network\n",
    "output_size = 64  # Number of output classes\n",
    "z_dim = 100  # Dimensionality of the generator's input noise\n",
    "\n",
    "# Simulate 100 batches of random tensors for audio, visual, labels, and z\n",
    "audio_loader = [torch.randn(10, input_size_audio) for _ in range(100)]\n",
    "visual_loader = [torch.randn(10, 3, input_size_visual, input_size_visual) for _ in range(100)]\n",
    "labels_loader = [torch.randint(0, output_size, (10,)) for _ in range(100)]\n",
    "z_loader = [torch.randn(10, z_dim) for _ in range(100)]\n",
    "\n",
    "# Train the model\n",
    "#train(model, audio_loader, visual_loader, labels_loader, z_loader, num_epochs=10)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, audio_input, visual_input):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # No need to calculate gradients during inference\n",
    "        final_pred, (final_audio_pred, final_visual_pred, *_) = model(audio_input, visual_input)\n",
    "        return final_audio_pred, final_visual_pred\n",
    "\n",
    "# Example evaluation\n",
    "#audio_input = torch.randn(1, input_size_audio)  # Single audio sample\n",
    "#visual_input = torch.randn(1, input_size_visual)  # Single visual sample\n",
    "#audio_pred, visual_pred = evaluate(model, audio_input, visual_input)\n",
    "#print(\"Audio Prediction:\", audio_pred)\n",
    "#print(\"Visual Prediction:\", visual_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b640e93a-557d-430f-9c45-1ece053186fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transf\n",
    "from data_feed import DataFeed, DataFeed_image_pos\n",
    "from build_net import resnet50, NN_beam_pred, MultinomialLogisticRegression, ImageFeatureExtractor, PosFeatureExtractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cbaad3-21e2-4ea2-8ae5-0e033a11f6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# or full reproducibility\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6891313-8761-41b4-974a-ea6a00d78d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Now you can use the `device` variable to move your model and data to the correct device\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa0ef78-25f1-4943-9f96-2dc1dce0b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the saved CSV files\n",
    "output_dir = \"./feature_IID/\"\n",
    "\n",
    "# Load one of the CSV files for EDA (e.g., user_0_outputs.csv)\n",
    "df = pd.read_csv(output_dir + \"user_0_pos_height_beam.csv\")\n",
    "\n",
    "# Quick overview of the data\n",
    "print(\"Data Overview:\")\n",
    "print(df.head())\n",
    "print(\"\\nData Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cc216a-d03d-4717-b2e0-d53f773a37e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "########################### Data pre-processing ########################\n",
    "########################################################################\n",
    "no_users = 20\n",
    "batch_size = 64\n",
    "img_resize = transf.Resize((224, 224))\n",
    "img_norm = transf.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "proc_pipe = transf.Compose(\n",
    "    [transf.ToPILImage(),\n",
    "     img_resize,\n",
    "     transf.ToTensor(),\n",
    "     img_norm]\n",
    ")\n",
    "dataset_dir = \"feature_IID/\"\n",
    "train_loaders = []\n",
    "test_loaders = []\n",
    "val_loaders = []\n",
    "\n",
    "for user_id in range(no_users):\n",
    "    train_dir = dataset_dir + f'user_{user_id}_pos_height_beam_train.csv'\n",
    "    val_dir = dataset_dir + f'user_{user_id}_pos_height_beam_val.csv'\n",
    "    test_dir = dataset_dir + f'user_{user_id}_pos_height_beam_test.csv'\n",
    "    \n",
    "    train_dataset = DataFeed_image_pos(train_dir, transform=proc_pipe)\n",
    "    val_dataset = DataFeed_image_pos(root_dir=val_dir, transform=proc_pipe)\n",
    "    test_dataset = DataFeed_image_pos(root_dir=test_dir, transform=proc_pipe)\n",
    "    \n",
    "    \n",
    "    train_loaders.append(DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              #num_workers=8,\n",
    "                              shuffle=True))\n",
    "    val_loaders.append(DataLoader(val_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            #num_workers=8,\n",
    "                            shuffle=False))\n",
    "    test_loaders.append(DataLoader(test_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            #num_workers=8,\n",
    "                            shuffle=False))\n",
    "print(\"All loadred are loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a4842a-bbbf-475f-bdbf-100f81e971ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Preperation#\n",
    "all_models = []\n",
    "available_modalities = [\"pos_height\", \"images\"]\n",
    "modality_size = {\"pos_height\": 128, \"images\": 128}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a9572a-e88f-431d-88a5-08aff378f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import random\n",
    "no_users = 20  # Example: Number of users\n",
    "available_modalities = [\"pos_height\", \"images\"]\n",
    "modality_size = {\"pos_height\": 128, \"images\": 128}\n",
    "group_definitions = {\n",
    "    1: [\"pos_height\"],        # Group 1: Only pos_height\n",
    "    2: [\"images\"],            # Group 2: Only images\n",
    "    3: [\"pos_height\", \"images\"]  # Group 3: Both modalities\n",
    "}\n",
    "\n",
    "# Assign each user to a group randomly\n",
    "weights = [0.2, 0.3, 0.5]  # Probabilities for groups 1, 2, and 3\n",
    "\n",
    "# Generate user_groups with weighted random choices\n",
    "user_groups = random.choices([1, 2, 3], weights=weights, k=no_users)\n",
    "\n",
    "# Assign modalities to users based on their group\n",
    "user_modalities = [group_definitions[group] for group in user_groups]\n",
    "\n",
    "# Compute output sizes for each user based on their modalities\n",
    "output_sizes = [sum(modality_size[modality] for modality in user_modality) for user_modality in user_modalities]\n",
    "\n",
    "# Store models (placeholders for actual models)\n",
    "all_models = []\n",
    "\n",
    "# Example output (for verification)\n",
    "print(f\"User Groups: {user_groups[:10]}\")  # Show first 10 users' groups\n",
    "print(f\"User Modalities: {user_modalities[:10]}\")  # Show first 10 users' modalities\n",
    "print(f\"Output Sizes: {output_sizes[:10]}\")  # Show first 10 users' output sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c95c2d-42a5-497f-8a05-a9e8934407dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_knopp(matrix, tol=1e-9, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Converts a given matrix to a doubly stochastic matrix using the Sinkhorn-Knopp algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "        matrix (np.ndarray): The input matrix to be transformed.\n",
    "        tol (float): The tolerance for convergence.\n",
    "        max_iter (int): Maximum number of iterations for convergence.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A doubly stochastic matrix.\n",
    "    \"\"\"\n",
    "    matrix = matrix.copy()\n",
    "    for _ in range(max_iter):\n",
    "        # Normalize rows\n",
    "        row_sums = matrix.sum(axis=1, keepdims=True)\n",
    "        matrix /= row_sums\n",
    "\n",
    "        # Normalize columns\n",
    "        col_sums = matrix.sum(axis=0, keepdims=True)\n",
    "        matrix /= col_sums\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.allclose(matrix.sum(axis=1), 1, atol=tol) and np.allclose(matrix.sum(axis=0), 1, atol=tol):\n",
    "            break\n",
    "\n",
    "    return matrix\n",
    "    \n",
    "def create_random_topology(num_users, similarity_matrix, edge_probability=0.3):\n",
    "    \"\"\"\n",
    "    Creates a connected random topology using NetworkX.\n",
    "    Returns the adjacency matrix.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        graph = nx.erdos_renyi_graph(num_users, edge_probability)\n",
    "        adjacency_matrix = nx.to_numpy_array(graph)\n",
    "        new_adj = np.multiply(adjacency_matrix, similarity_matrix)\n",
    "        new_graph = nx.from_numpy_array(new_adj)\n",
    "        if nx.is_connected(new_graph):\n",
    "            break\n",
    "\n",
    "    # Convert graph to adjacency matrix\n",
    "    adjacency_matrix = nx.to_numpy_array(new_graph)\n",
    "    return adjacency_matrix\n",
    "\n",
    "def prepare_mixing_matrices(adjacency_matrix, similarity_matrices):\n",
    "    \"\"\"\n",
    "    Computes a mixing matrix for each modality by multiplying the adjacency matrix \n",
    "    with the similarity matrix for that modality.\n",
    "    Returns a dictionary of mixing matrices.\n",
    "    \"\"\"\n",
    "    adjacency_matrices = {}\n",
    "    mixing_matrices = {}\n",
    "    for modality, similarity_matrix in similarity_matrices.items():\n",
    "        # Element-wise multiplication of adjacency and similarity matrices\n",
    "        combined_matrix = adjacency_matrix * similarity_matrix\n",
    "        adjacency_matrices[modality] = combined_matrix\n",
    "        \n",
    "        # Normalize to create a doubly matrix\n",
    "        mixing_matrix = sinkhorn_knopp(combined_matrix)\n",
    "        \n",
    "        \n",
    "        mixing_matrices[modality] = mixing_matrix\n",
    "    \n",
    "    return mixing_matrices, adjacency_matrices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e735154-ee14-48d4-b967-90c61895a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random connected topology\n",
    "#adjacency_matrix = create_random_topology(no_users, edge_probability=0.3)\n",
    "# Initialize adjacency matrix\n",
    "similarity_matrix = np.zeros((no_users, no_users), dtype=int)\n",
    "\n",
    "# Construct the adjacency matrix\n",
    "for i in range(no_users):\n",
    "    for j in range(no_users):\n",
    "        if i != j:  # No self-loops\n",
    "            # Check if users i and j share any modalities\n",
    "            if set(user_modalities[i]) & set(user_modalities[j]):\n",
    "                similarity_matrix[i, j] = 1\n",
    "\n",
    "# Display the adjacency matrix\n",
    "print(\"Adjacency Matrix:\")\n",
    "print(similarity_matrix)\n",
    "\n",
    "# Prepare mixing matrices for each modality\n",
    "#mixing_matrices, adjacency_matrices = prepare_mixing_matrices(adjacency_matrix, similarity_matrices)\n",
    "adjacency_matrix = create_random_topology(20, similarity_matrix, edge_probability=0.3)\n",
    "print(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4df557-5b41-4333-b208-bf6435842c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_knopp(matrix, tol=1e-9, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Converts a given matrix to a doubly stochastic matrix using the Sinkhorn-Knopp algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "        matrix (np.ndarray): The input matrix to be transformed.\n",
    "        tol (float): The tolerance for convergence.\n",
    "        max_iter (int): Maximum number of iterations for convergence.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A doubly stochastic matrix.\n",
    "    \"\"\"\n",
    "    matrix = matrix.copy()\n",
    "    for _ in range(max_iter):\n",
    "        # Normalize rows\n",
    "        row_sums = matrix.sum(axis=1, keepdims=True)\n",
    "        matrix /= row_sums\n",
    "\n",
    "        # Normalize columns\n",
    "        col_sums = matrix.sum(axis=0, keepdims=True)\n",
    "        matrix /= col_sums\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.allclose(matrix.sum(axis=1), 1, atol=tol) and np.allclose(matrix.sum(axis=0), 1, atol=tol):\n",
    "            break\n",
    "\n",
    "    return matrix\n",
    "    \n",
    "def create_random_topology(num_users, similarity_matrix, edge_probability=0.3):\n",
    "    \"\"\"\n",
    "    Creates a connected random topology using NetworkX.\n",
    "    Returns the adjacency matrix.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        graph = nx.erdos_renyi_graph(num_users, edge_probability)\n",
    "        adjacency_matrix = nx.to_numpy_array(graph)\n",
    "        new_adj = np.multiply(adjacency_matrix, similarity_matrix)\n",
    "        new_graph = nx.from_numpy_array(new_adj)\n",
    "        if nx.is_connected(new_graph):\n",
    "            break\n",
    "\n",
    "    # Convert graph to adjacency matrix\n",
    "    adjacency_matrix = nx.to_numpy_array(new_graph)\n",
    "    return adjacency_matrix\n",
    "\n",
    "def prepare_mixing_matrices(adjacency_matrix, similarity_matrices):\n",
    "    \"\"\"\n",
    "    Computes a mixing matrix for each modality by multiplying the adjacency matrix \n",
    "    with the similarity matrix for that modality.\n",
    "    Returns a dictionary of mixing matrices.\n",
    "    \"\"\"\n",
    "    adjacency_matrices = {}\n",
    "    mixing_matrices = {}\n",
    "    for modality, similarity_matrix in similarity_matrices.items():\n",
    "        # Element-wise multiplication of adjacency and similarity matrices\n",
    "        combined_matrix = adjacency_matrix * similarity_matrix\n",
    "        adjacency_matrices[modality] = combined_matrix\n",
    "        \n",
    "        # Normalize to create a doubly matrix\n",
    "        mixing_matrix = sinkhorn_knopp(combined_matrix)\n",
    "        \n",
    "        \n",
    "        mixing_matrices[modality] = mixing_matrix\n",
    "    \n",
    "    return mixing_matrices, adjacency_matrices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a0e1f-5445-4181-bce0-4bc282edde8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the graph\n",
    "# Define colors for the groups\n",
    "group_colors = {1: 'red', 2: 'green', 3: 'blue'}\n",
    "node_colors = [group_colors[group] for group in user_groups]\n",
    "G = nx.from_numpy_array(similarity_matrix)\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, edge_color='gray', node_size=1000, node_color=node_colors, font_size=20, font_color='black')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df54d0c1-b6b7-409c-89fe-789f41818e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the graph\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "G = nx.from_numpy_array(adjacency_matrix)\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, edge_color='gray', node_size=1000, node_color=node_colors, font_size=20, font_color='black')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "print(user_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56fdf93-74f1-4eab-9bba-2aadb6cc097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity matrices\n",
    "adj_per_modality = {}\n",
    "for modality in available_modalities:\n",
    "    adj = np.zeros((no_users, no_users))\n",
    "    for node in range(no_users):\n",
    "        for neighbor in G.neighbors(node):\n",
    "            if modality in user_modalities[neighbor] and modality in user_modalities[node]:\n",
    "                adj[node, neighbor] = 1.    \n",
    "    adj_per_modality[modality] = adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb316e-d5ab-4c2f-bd38-6732228b7e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_modality = nx.from_numpy_array(adj_per_modality[\"images\"])\n",
    "pos = nx.spring_layout(G_modality)\n",
    "nx.draw(G_modality, pos, with_labels=True, edge_color='gray', node_size=1000, node_color=node_colors, font_size=20, font_color='black')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317b907-f7ae-42c0-8921-f31b2cee9f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_mixing_matrix(Adj, method=\"metropolis\"):\n",
    "    n = Adj.shape[0]\n",
    "    W = np.zeros((n, n))  # Initialize weight matrix\n",
    "\n",
    "    for i in range(n):\n",
    "        degree_i = np.sum(Adj[i, :])\n",
    "\n",
    "        for j in range(n):\n",
    "            if Adj[i, j] == 1.0:\n",
    "                degree_j = np.sum(Adj[j, :])\n",
    "    \n",
    "                if method == \"metropolis\":\n",
    "                    W[i, j] = 1 / (max(degree_i, degree_j) + 1)\n",
    "                elif method == \"uniform\":\n",
    "                    W[i, j] = 1 / degree_i\n",
    "\n",
    "        # Diagonal weight\n",
    "        W[i, i] = 1 - np.sum(W[i, :])\n",
    "\n",
    "    return W\n",
    "\n",
    "mixing_matrices = {}\n",
    "for modality in available_modalities:\n",
    "    mixing_matrices[modality] = construct_mixing_matrix(adj_per_modality[modality], method=\"metropolis\")\n",
    "    print(np.sum(mixing_matrices[modality], 0))\n",
    "    print(np.sum(mixing_matrices[modality], 1))\n",
    "    lamb = np.linalg.eigvals(mixing_matrices[modality])\n",
    "    lamb.sort()\n",
    "    print(lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd6829f-6252-40e3-97d0-ad0ea2d4882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_modality = nx.from_numpy_array(adj_per_modality[\"pos_height\"])\n",
    "pos = nx.spring_layout(G_modality)\n",
    "largest_cc = max(nx.connected_components(G_modality), key=len)\n",
    "\n",
    "# Convert to sorted list of indices\n",
    "connected_nodes = sorted(largest_cc)\n",
    "\n",
    "# Extract the submatrix corresponding to the connected subgraph\n",
    "W_reduced = mixing_matrices[\"pos_height\"][np.ix_(connected_nodes, connected_nodes)]\n",
    "lamb = np.linalg.eigvals(W_reduced)\n",
    "lamb.sort()\n",
    "print(lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441abedd-5799-4950-950b-f23acae1f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "\n",
    "input_size_audio = 4  # Example size, modify according to your data\n",
    "input_size_visual = 224  # Example size, modify according to your data\n",
    "hidden_size = 256  # Hidden size of the network\n",
    "output_size = 64  # Number of output classes\n",
    "z_dim = 100  # Dimensionality of the generator's input noise\n",
    "optimizers = []\n",
    "all_models = []\n",
    "\n",
    "classifier_optimizers = []\n",
    "for user_id in range(no_users):\n",
    "    if \"images\" in user_modalities[user_id] and \"pos_height\" in user_modalities[user_id]:\n",
    "        user_model = MultiModalNetwork(\n",
    "    input_size_audio=input_size_audio,\n",
    "    input_size_visual=input_size_visual,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_size,\n",
    "    z_dim=z_dim\n",
    ").to(device)\n",
    "    elif \"pos_height\" in user_modalities[user_id]:\n",
    "        user_model = MultiModalNetwork(\n",
    "    input_size_audio=input_size_audio,\n",
    "    input_size_visual=None,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_size,\n",
    "    z_dim=z_dim\n",
    ").to(device)\n",
    "    elif \"images\" in user_modalities[user_id]:\n",
    "        user_model = MultiModalNetwork(\n",
    "    input_size_audio=None,\n",
    "    input_size_visual=input_size_visual,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_size,\n",
    "    z_dim=z_dim\n",
    ").to(device)\n",
    "    local_optimizer = optim.Adam(user_model.parameters(), lr=lr)\n",
    "\n",
    "    all_models.append(user_model)\n",
    "    optimizers.append(local_optimizer)\n",
    "base_models = MultiModalNetwork(\n",
    "    input_size_audio=input_size_audio,\n",
    "    input_size_visual=input_size_visual,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_size,\n",
    "    z_dim=z_dim\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0b559-d49b-4d97-be81-6218622f33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def per_modelaity_decentralized_aggregation(user_models, mixing_matrices, available_modalities, user_modalities):\n",
    "    num_users = len(user_models)\n",
    "    with torch.no_grad():\n",
    "        for modality in available_modalities:\n",
    "            mixing_matrix = mixing_matrices[modality]\n",
    "            \n",
    "            # Collect model parameters for feature extractors and classifiers\n",
    "            aggregated_feature_extractors = []\n",
    "            aggregated_common_classifiers = []\n",
    "            aggregated_specific_classifiers = []\n",
    "            \n",
    "            updates_feature_extractors = []\n",
    "            updates_common_classifiers = []\n",
    "            updates_specific_classifiers = []\n",
    "            \n",
    "            for user_id, user_model in enumerate(user_models):\n",
    "                if modality in user_modalities[user_id]:\n",
    "                    # Extract feature extractor and classifier parameters\n",
    "                    feature_extractor = user_model.audio_feature_extractor if modality == 'pos_height' else user_model.visual_feature_extractor\n",
    "                    specific_classifier = user_model.audio_specific_classifier if modality == 'pos_height' else user_model.visual_specific_classifier\n",
    "                    common_classifier = user_model.common_classifier\n",
    "                    # Convert to vector\n",
    "                    aggregated_feature_extractors.append(torch.nn.utils.parameters_to_vector(feature_extractor.parameters()))\n",
    "                    aggregated_common_classifiers.append(torch.nn.utils.parameters_to_vector(common_classifier.parameters()))\n",
    "                    aggregated_specific_classifiers.append(torch.nn.utils.parameters_to_vector(specific_classifier.parameters()))\n",
    "                    \n",
    "                    # Initialize update vectors\n",
    "                    updates_feature_extractors.append(torch.zeros_like(aggregated_feature_extractors[-1]))\n",
    "                    updates_common_classifiers.append(torch.zeros_like(aggregated_common_classifiers[-1]))\n",
    "                    updates_specific_classifiers.append(torch.zeros_like(aggregated_specific_classifiers[-1]))\n",
    "                else:\n",
    "                    aggregated_feature_extractors.append(0)\n",
    "                    aggregated_common_classifiers.append(0)\n",
    "                    aggregated_specific_classifiers.append(0)\n",
    "                    \n",
    "                    updates_feature_extractors.append(0)\n",
    "                    updates_common_classifiers.append(0)\n",
    "                    updates_specific_classifiers.append(0)\n",
    "            \n",
    "            # Aggregate models using the mixing matrix\n",
    "            for i in range(num_users):\n",
    "                for j in range(num_users):\n",
    "                    if mixing_matrix[i, j] > 0:\n",
    "                        updates_feature_extractors[i] += mixing_matrix[i, j] * aggregated_feature_extractors[j]\n",
    "                        updates_common_classifiers[i] += mixing_matrix[i, j] * aggregated_common_classifiers[j]\n",
    "                        updates_specific_classifiers[i] += mixing_matrix[i, j] * aggregated_specific_classifiers[j]\n",
    "            \n",
    "            # Update user models with aggregated parameters\n",
    "            for user_id in range(num_users):\n",
    "                if modality in user_modalities[user_id]:\n",
    "                    feature_extractor = user_models[user_id].audio_feature_extractor if modality == 'pos_height' else user_models[user_id].visual_feature_extractor\n",
    "                    common_classifier = user_models[user_id].common_classifier\n",
    "                    specific_classifier = user_models[user_id].audio_specific_classifier if modality == 'pos_height' else user_models[user_id].visual_specific_classifier\n",
    "                    \n",
    "                    torch.nn.utils.vector_to_parameters(updates_feature_extractors[user_id], feature_extractor.parameters())\n",
    "                    torch.nn.utils.vector_to_parameters(updates_common_classifiers[user_id], common_classifier.parameters())\n",
    "                    torch.nn.utils.vector_to_parameters(updates_specific_classifiers[user_id], specific_classifier.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd3ed87-b01b-4f8c-9886-5526789eba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_local_model(local_modalities, model, data_loader, optimizer, num_epochs=10, alpha0=1.0, alpha1=1.0, alpha2=1.0, alpha3=1.0, alpha_gen=1.0, alpha_kd=1.0, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Trains a local multi-modal model using decentralized modalities and computes accuracy.\n",
    "\n",
    "    Args:\n",
    "        model (MultiModalNetwork): Multi-modal classification model.\n",
    "        data_loaders (dict): Dictionary containing modality-specific data loaders.\n",
    "            Example: {\"audio\": audio_loader, \"visual\": visual_loader, \"labels\": labels_loader, \"z\": z_loader}\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for the model parameters.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        alpha1, alpha2, alpha_gen, alpha_kd (float): Loss weighting factors.\n",
    "        device (str): Device to use (\"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "        training_losses (list): List of training losses per epoch.\n",
    "        training_accuracies (list): List of training accuracies per epoch.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Freeze feature extractors but keep classifiers trainable\n",
    "    if hasattr(model, \"audio_feature_extractor\"):\n",
    "        for param in model.audio_feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "    if hasattr(model, \"visual_feature_extractor\"):\n",
    "        for param in model.visual_feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    training_losses = []\n",
    "    training_accuracies = []\n",
    "    print(num_epochs)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for batch in data_loader:\n",
    "            inputs, labels = batch\n",
    "            z = torch.randn(labels.shape[0], 100).to(device)\n",
    "            # Prepare input data for selected modalities\n",
    "            modality_inputs = {mod: inputs[mod].to(device) for mod in local_modalities}\n",
    "            # Assuming 'inputs' is a dictionary containing the data for all modalities\n",
    "            modality_inputs = {mod: inputs[mod].to(device) for mod in local_modalities}\n",
    "            \n",
    "            # Initialize inputs as None\n",
    "            audio_input = None\n",
    "            visual_input = None\n",
    "            \n",
    "            # Split based on available modalities\n",
    "            if \"pos_height\" in local_modalities:\n",
    "                audio_input = modality_inputs[\"pos_height\"]\n",
    "            if \"images\" in local_modalities:\n",
    "                visual_input = modality_inputs[\"images\"]\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "            # Move inputs to device\n",
    "            audio_input = audio_input.to(device) if audio_input is not None else None\n",
    "            visual_input = visual_input.to(device) if visual_input is not None else None\n",
    "            labels = labels.to(device)\n",
    "            z = z.to(device) if z is not None else None\n",
    "            final_pred = model(audio_input=audio_input, visual_input=visual_input, z=z)\n",
    "\n",
    "\n",
    "            # Forward pass and compute loss\n",
    "            loss_outputs = model.compute_loss(audio_input, visual_input, labels, z, alpha0, alpha1, alpha2, alpha3, alpha_gen, alpha_kd)\n",
    "            total_loss, classification_loss, similarity_loss, auxiliary_loss, difference_loss, generation_loss, kd_loss = loss_outputs\n",
    "\n",
    "            #print(f\"Total Loss: {total_loss}\")\n",
    "            #print(f\"Classification Loss: {classification_loss}\")\n",
    "            #print(f\"Similarity Loss: {similarity_loss}\")\n",
    "            #print(f\"Auxiliary Loss: {auxiliary_loss}\")\n",
    "            #print(f\"Difference Loss: {difference_loss}\")\n",
    "            #print(f\"Generation Loss: {generation_loss}\")\n",
    "            #print(f\"Knowledge Distillation Loss: {kd_loss}\")\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()  # Update the model parameters\n",
    "\n",
    "            running_loss += total_loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            with torch.no_grad():\n",
    "                predictions,_ = model(audio_input, visual_input, z)\n",
    "                _, predicted_labels = torch.max(predictions, dim=1)\n",
    "                correct_predictions += (predicted_labels == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "        # Compute epoch loss and accuracy\n",
    "        avg_loss = running_loss / len(data_loader)\n",
    "        accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "        training_losses.append(avg_loss)\n",
    "        training_accuracies.append(accuracy)\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return training_losses, training_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327c51a-a71f-4724-b896-29e02967db31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def validate_user_models(user_id, model, data_loader, local_modalities, alpha0=1.0, alpha1=1.0, alpha2=1.0, alpha3=1.0, alpha_gen=1.0, alpha_kd=1.0, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Validates a trained multi-modal model using the data from different modalities.\n",
    "\n",
    "    Args:\n",
    "        user_id (int): User identifier.\n",
    "        model (MultiModalNetwork): Multi-modal classification model.\n",
    "        data_loader (DataLoader): DataLoader for the validation set.\n",
    "        criterion (nn.CrossEntropyLoss): Loss function.\n",
    "        local_modalities (list): Modalities to use (e.g., ['audio', 'visual']).\n",
    "        device (torch.device): Device (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        dict: Validation loss and accuracy.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs, labels = batch\n",
    "            z = torch.randn(labels.shape[0], 100).to(device)\n",
    "            \n",
    "            # Prepare input data for selected modalities\n",
    "            modality_inputs = {mod: inputs[mod].to(device) for mod in local_modalities}\n",
    "            \n",
    "            # Initialize inputs as None\n",
    "            audio_input = None\n",
    "            visual_input = None\n",
    "            \n",
    "            # Split based on available modalities\n",
    "            if \"pos_height\" in local_modalities:\n",
    "                audio_input = modality_inputs[\"pos_height\"]\n",
    "            if \"images\" in local_modalities:\n",
    "                visual_input = modality_inputs[\"images\"]\n",
    "            \n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs, _ = model(audio_input=audio_input, visual_input=visual_input, z=z)\n",
    "            loss_outputs = model.compute_loss(audio_input, visual_input, labels, z, alpha0, alpha1, alpha2, alpha3, alpha_gen, alpha_kd)\n",
    "            loss, classification_loss, similarity_loss, auxiliary_loss, difference_loss, generation_loss, kd_loss = loss_outputs\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Compute average loss and accuracy\n",
    "    avg_loss = total_loss / total_samples if total_samples > 0 else 0.0\n",
    "    accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "    print(f\"User {user_id + 1} - Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return {\"loss\": avg_loss, \"accuracy\": accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da353892-814e-4f25-a726-e5bfd8389360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss_with_l2(model, logits, targets, l2_strength=1e-3):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(logits, targets)\n",
    "    l2_reg = sum(param.pow(2).sum() for param in model.parameters())\n",
    "    \n",
    "    return loss + l2_strength * l2_reg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08ecb69-5a71-4045-9a1e-3375d81fee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Hyperparameters\n",
    "lambda_reg = 0.01\n",
    "eta = 0.001\n",
    "alpha = 1e-1\n",
    "num_epochs=1\n",
    "alpha0 = 1e-2\n",
    "alpha1=alpha\n",
    "alpha2=alpha * 1e-3\n",
    "alpha3 = alpha \n",
    "alpha_gen=alpha\n",
    "alpha_kd=alpha\n",
    "beta = 1.0\n",
    "# Dictionaries to store metrics\n",
    "group_train_loss_histories = {1: [], 2: [], 3: []}\n",
    "group_train_accuracy_histories = {1: [], 2: [], 3: []}\n",
    "group_val_loss_histories = {1: [], 2: [], 3: []}\n",
    "group_val_accuracy_histories = {1: [], 2: [], 3: []}\n",
    "\n",
    "global_rounds = 100\n",
    "local_epochs = 1\n",
    "\n",
    "# Decentralized Federated Learning Loop\n",
    "for round_num in range(global_rounds):\n",
    "    print(f\"Global Round {round_num + 1}\")\n",
    "\n",
    "    # Decentralized aggregation step\n",
    "    per_modelaity_decentralized_aggregation(all_models, mixing_matrices, available_modalities, user_modalities)\n",
    "\n",
    "    # Temporary storage for this round\n",
    "    epoch_group_train_losses = {1: [], 2: [], 3: []}\n",
    "    epoch_group_train_accuracies = {1: [], 2: [], 3: []}\n",
    "    epoch_group_val_losses = {1: [], 2: [], 3: []}\n",
    "    epoch_group_val_accuracies = {1: [], 2: [], 3: []}\n",
    "\n",
    "    # Training phase\n",
    "    for user_id in range(no_users):\n",
    "        print(f\"Training model for User {user_id + 1}\")\n",
    "        user_models = all_models[user_id]\n",
    "        group = user_groups[user_id]\n",
    "\n",
    "        # Train local model for the user's available modalities\n",
    "        train_loss, train_accuracy = train_local_model(user_modalities[user_id], user_models, train_loaders[user_id], optimizers[user_id], num_epochs, alpha0, alpha1, alpha2, alpha3, alpha_gen, alpha_kd, device)\n",
    "\n",
    "        # Store in group-wise metrics\n",
    "        epoch_group_train_losses[group].append(train_loss)\n",
    "        epoch_group_train_accuracies\n",
    "    # Validation phase\n",
    "    for user_id in range(no_users):\n",
    "        user_models = all_models[user_id]\n",
    "        val_dict = validate_user_models(\n",
    "            user_id, user_models, \n",
    "            val_loaders[user_id], \n",
    "            user_modalities[user_id], \n",
    "            alpha0, alpha1, alpha2, alpha3, alpha_gen, \n",
    "            alpha_kd, device)\n",
    "        group = user_groups[user_id]\n",
    "        epoch_group_val_losses[group].append(val_dict[\"loss\"])\n",
    "        epoch_group_val_accuracies[group].append(val_dict[\"accuracy\"])\n",
    "\n",
    "    # Store final metrics for each group\n",
    "    for group in [1, 2, 3]:\n",
    "        group_train_loss_histories[group].append(epoch_group_train_losses[group])\n",
    "        group_train_accuracy_histories[group].append(epoch_group_train_accuracies[group])\n",
    "        group_val_loss_histories[group].append(epoch_group_val_losses[group])\n",
    "        group_val_accuracy_histories[group].append(epoch_group_val_accuracies[group])\n",
    "\n",
    "    # Print final results for this round\n",
    "    print(f\"---- Global Round {round_num + 1} Metrics ----\")\n",
    "    for group in [1, 2, 3]:\n",
    "        print(f\"  Group {group} - Train Loss: {np.mean(group_train_loss_histories[group][-1]):.4f}, Train Accuracy: {np.mean(group_train_accuracy_histories[group][-1]):.4f}\")\n",
    "        print(f\"  Group {group} - Val Loss: {np.mean(group_val_loss_histories[group][-1]):.4f}, Val Accuracy: {np.mean(group_val_accuracy_histories[group][-1]):.4f}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817eb92-3cc9-4558-a7e0-5394a8fb90ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25#global_rounds\n",
    "# Convert metrics to numpy arrays for easy manipulation\n",
    "group_train_loss_histories = {k: np.array(v) for k, v in group_train_loss_histories.items()}\n",
    "group_train_accuracy_histories = {k: np.array(v) for k, v in group_train_accuracy_histories.items()}\n",
    "group_val_loss_histories = {k: np.array(v) for k, v in group_val_loss_histories.items()}\n",
    "group_val_accuracy_histories = {k: np.array(v) for k, v in group_val_accuracy_histories.items()}\n",
    "\n",
    "# Handle potential one-dimensional arrays\n",
    "group_train_loss_mean = {k: v.mean(axis=1) if v.ndim > 1 else v for k, v in group_train_loss_histories.items()}\n",
    "group_train_loss_std = {k: v.std(axis=1) if v.ndim > 1 else np.zeros_like(v) for k, v in group_train_loss_histories.items()}\n",
    "group_val_loss_mean = {k: v.mean(axis=1) if v.ndim > 1 else v for k, v in group_val_loss_histories.items()}\n",
    "group_val_loss_std = {k: v.std(axis=1) if v.ndim > 1 else np.zeros_like(v) for k, v in group_val_loss_histories.items()}\n",
    "\n",
    "group_train_acc_mean = {k: v.mean(axis=1) if v.ndim > 1 else v for k, v in group_train_accuracy_histories.items()}\n",
    "group_train_acc_std = {k: v.std(axis=1) if v.ndim > 1 else np.zeros_like(v) for k, v in group_train_accuracy_histories.items()}\n",
    "group_val_acc_mean = {k: v.mean(axis=1) if v.ndim > 1 else v for k, v in group_val_accuracy_histories.items()}\n",
    "group_val_acc_std = {k: v.std(axis=1) if v.ndim > 1 else np.zeros_like(v) for k, v in group_val_accuracy_histories.items()}\n",
    "\n",
    "# Combined Plot for All Modalities\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "for group in [1, 2, 3]:\n",
    "    plt.plot(range(1, num_epochs + 1), group_train_loss_mean[group], label=f\"Group {group} Train Loss\")\n",
    "    plt.fill_between(range(1, num_epochs + 1), \n",
    "                     group_train_loss_mean[group] - group_train_loss_std[group], \n",
    "                     group_train_loss_mean[group] + group_train_loss_std[group], \n",
    "                     alpha=0.2)\n",
    "    plt.plot(range(1, num_epochs + 1), group_val_loss_mean[group], label=f\"Group {group} Validation Loss\", linestyle=\"dashed\")\n",
    "    plt.fill_between(range(1, num_epochs + 1), \n",
    "                     group_val_loss_mean[group] - group_val_loss_std[group], \n",
    "                     group_val_loss_mean[group] + group_val_loss_std[group], \n",
    "                     alpha=0.2)\n",
    "\n",
    "plt.title(\"Loss over Epochs for All Groups\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Accuracy Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "for group in [1, 2, 3]:\n",
    "    plt.plot(range(1, num_epochs + 1), group_train_acc_mean[group], label=f\"Group {group} Train Accuracy\")\n",
    "    plt.fill_between(range(1, num_epochs + 1), \n",
    "                     group_train_acc_mean[group] - group_train_acc_std[group], \n",
    "                     group_train_acc_mean[group] + group_train_acc_std[group], \n",
    "                     alpha=0.2)\n",
    "    plt.plot(range(1, num_epochs + 1), group_val_acc_mean[group], label=f\"Group {group} Validation Accuracy\", linestyle=\"dashed\")\n",
    "    plt.fill_between(range(1, num_epochs + 1), \n",
    "                     group_val_acc_mean[group] - group_val_acc_std[group], \n",
    "                     group_val_acc_mean[group] + group_val_acc_std[group], \n",
    "                     alpha=0.2)\n",
    "\n",
    "plt.title(\"Accuracy over Epochs for All Groups\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([0, 1])  # Ensure y-axis is between 0 and 1 for accuracy\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b6f102-064b-4bc7-9292-b90324babb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert numpy arrays to lists for serialization\n",
    "data_to_save = {\n",
    "    \"group_train_loss_mean\": {k: v.tolist() for k, v in group_train_loss_mean.items()},\n",
    "    \"group_train_loss_std\": {k: v.tolist() for k, v in group_train_loss_std.items()},\n",
    "    \"group_val_loss_mean\": {k: v.tolist() for k, v in group_val_loss_mean.items()},\n",
    "    \"group_val_loss_std\": {k: v.tolist() for k, v in group_val_loss_std.items()},\n",
    "    \"group_train_acc_mean\": {k: v.tolist() for k, v in group_train_acc_mean.items()},\n",
    "    \"group_train_acc_std\": {k: v.tolist() for k, v in group_train_acc_std.items()},\n",
    "    \"group_val_acc_mean\": {k: v.tolist() for k, v in group_val_acc_mean.items()},\n",
    "    \"group_val_acc_std\": {k: v.tolist() for k, v in group_val_acc_std.items()}\n",
    "}\n",
    "\n",
    "with open(\"KL_metrics_IID.json\", \"w\") as f:\n",
    "    json.dump(data_to_save, f)\n",
    "print(\"KL_metrics_IID.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
