{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b537c0-5308-494e-b479-df1ed1fd34b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transf\n",
    "from data_feed import DataFeed, DataFeed_image_pos\n",
    "from build_net import resnet50, NN_beam_pred, MultinomialLogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa17e9f-2958-489b-af3e-4dab1dfe1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# or full reproducibility\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38bfa9a-c6ef-450a-9180-61eb294a2ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Now you can use the `device` variable to move your model and data to the correct device\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be724f96-0c63-47c8-ad68-1b617c05d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the saved CSV files\n",
    "output_dir = \"./feature_IID/\"\n",
    "\n",
    "# Load one of the CSV files for EDA (e.g., user_0_outputs.csv)\n",
    "df = pd.read_csv(output_dir + \"user_0_pos_height_beam.csv\")\n",
    "\n",
    "# Quick overview of the data\n",
    "print(\"Data Overview:\")\n",
    "print(df.head())\n",
    "print(\"\\nData Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bf9b52-16e1-4ec8-acd5-1cbd19cab71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "########################### Data pre-processing ########################\n",
    "########################################################################\n",
    "no_users = 20\n",
    "batch_size = 64\n",
    "img_resize = transf.Resize((224, 224))\n",
    "img_norm = transf.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "proc_pipe = transf.Compose(\n",
    "    [transf.ToPILImage(),\n",
    "     img_resize,\n",
    "     transf.ToTensor(),\n",
    "     img_norm]\n",
    ")\n",
    "dataset_dir = \"feature_IID/\"\n",
    "train_loaders = []\n",
    "test_loaders = []\n",
    "val_loaders = []\n",
    "\n",
    "for user_id in range(no_users):\n",
    "    train_dir = dataset_dir + f'user_{user_id}_pos_height_beam_train.csv'\n",
    "    val_dir = dataset_dir + f'user_{user_id}_pos_height_beam_val.csv'\n",
    "    test_dir = dataset_dir + f'user_{user_id}_pos_height_beam_test.csv'\n",
    "    \n",
    "    train_dataset = DataFeed_image_pos(train_dir, transform=proc_pipe)\n",
    "    val_dataset = DataFeed_image_pos(root_dir=val_dir, transform=proc_pipe)\n",
    "    test_dataset = DataFeed_image_pos(root_dir=test_dir, transform=proc_pipe)\n",
    "    \n",
    "    \n",
    "    train_loaders.append(DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              #num_workers=8,\n",
    "                              shuffle=False))\n",
    "    val_loaders.append(DataLoader(val_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            #num_workers=8,\n",
    "                            shuffle=False))\n",
    "    test_loaders.append(DataLoader(test_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            #num_workers=8,\n",
    "                            shuffle=False))\n",
    "print(\"All loadred are loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2aeb40-49ab-4536-a119-acf212a41a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Preperation#\n",
    "all_models = []\n",
    "available_modalities = [\"pos_height\", \"images\"]\n",
    "modality_size = {\"pos_height\": 128, \"images\": 128}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5627ec-2bc5-406a-bed8-a058e4a728b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import random\n",
    "no_users = 20  # Example: Number of users\n",
    "available_modalities = [\"pos_height\", \"images\"]\n",
    "modality_size = {\"pos_height\": 128, \"images\": 128}\n",
    "group_definitions = {\n",
    "    1: [\"pos_height\"],        # Group 1: Only pos_height\n",
    "    2: [\"images\"],            # Group 2: Only images\n",
    "    3: [\"pos_height\", \"images\"]  # Group 3: Both modalities\n",
    "}\n",
    "\n",
    "# Assign each user to a group randomly\n",
    "weights = [0.2, 0.3, 0.5]  # Probabilities for groups 1, 2, and 3\n",
    "\n",
    "# Generate user_groups with weighted random choices\n",
    "user_groups = random.choices([1, 2, 3], weights=weights, k=no_users)\n",
    "\n",
    "# Assign modalities to users based on their group\n",
    "user_modalities = [group_definitions[group] for group in user_groups]\n",
    "\n",
    "# Compute output sizes for each user based on their modalities\n",
    "output_sizes = [sum(modality_size[modality] for modality in user_modality) for user_modality in user_modalities]\n",
    "\n",
    "# Store models (placeholders for actual models)\n",
    "all_models = []\n",
    "\n",
    "# Example output (for verification)\n",
    "print(f\"User Groups: {user_groups[:10]}\")  # Show first 10 users' groups\n",
    "print(f\"User Modalities: {user_modalities[:10]}\")  # Show first 10 users' modalities\n",
    "print(f\"Output Sizes: {output_sizes[:10]}\")  # Show first 10 users' output sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed4d5c-f244-40b5-9036-25a31edc6e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_knopp(matrix, tol=1e-9, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Converts a given matrix to a doubly stochastic matrix using the Sinkhorn-Knopp algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "        matrix (np.ndarray): The input matrix to be transformed.\n",
    "        tol (float): The tolerance for convergence.\n",
    "        max_iter (int): Maximum number of iterations for convergence.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A doubly stochastic matrix.\n",
    "    \"\"\"\n",
    "    matrix = matrix.copy()\n",
    "    for _ in range(max_iter):\n",
    "        # Normalize rows\n",
    "        row_sums = matrix.sum(axis=1, keepdims=True)\n",
    "        matrix /= row_sums\n",
    "\n",
    "        # Normalize columns\n",
    "        col_sums = matrix.sum(axis=0, keepdims=True)\n",
    "        matrix /= col_sums\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.allclose(matrix.sum(axis=1), 1, atol=tol) and np.allclose(matrix.sum(axis=0), 1, atol=tol):\n",
    "            break\n",
    "\n",
    "    return matrix\n",
    "    \n",
    "def create_random_topology(num_users, similarity_matrix, edge_probability=0.3):\n",
    "    \"\"\"\n",
    "    Creates a connected random topology using NetworkX.\n",
    "    Returns the adjacency matrix.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        graph = nx.erdos_renyi_graph(num_users, edge_probability)\n",
    "        adjacency_matrix = nx.to_numpy_array(graph)\n",
    "        new_adj = np.multiply(adjacency_matrix, similarity_matrix)\n",
    "        new_graph = nx.from_numpy_array(new_adj)\n",
    "        if nx.is_connected(new_graph):\n",
    "            break\n",
    "\n",
    "    # Convert graph to adjacency matrix\n",
    "    adjacency_matrix = nx.to_numpy_array(new_graph)\n",
    "    return adjacency_matrix\n",
    "\n",
    "def prepare_mixing_matrices(adjacency_matrix, similarity_matrices):\n",
    "    \"\"\"\n",
    "    Computes a mixing matrix for each modality by multiplying the adjacency matrix \n",
    "    with the similarity matrix for that modality.\n",
    "    Returns a dictionary of mixing matrices.\n",
    "    \"\"\"\n",
    "    adjacency_matrices = {}\n",
    "    mixing_matrices = {}\n",
    "    for modality, similarity_matrix in similarity_matrices.items():\n",
    "        # Element-wise multiplication of adjacency and similarity matrices\n",
    "        combined_matrix = adjacency_matrix * similarity_matrix\n",
    "        adjacency_matrices[modality] = combined_matrix\n",
    "        \n",
    "        # Normalize to create a doubly matrix\n",
    "        mixing_matrix = sinkhorn_knopp(combined_matrix)\n",
    "        \n",
    "        \n",
    "        mixing_matrices[modality] = mixing_matrix\n",
    "    \n",
    "    return mixing_matrices, adjacency_matrices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df528594-e32b-4cf3-8dd4-af4e5a428428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random connected topology\n",
    "#adjacency_matrix = create_random_topology(no_users, edge_probability=0.3)\n",
    "# Initialize adjacency matrix\n",
    "similarity_matrix = np.zeros((no_users, no_users), dtype=int)\n",
    "\n",
    "# Construct the adjacency matrix\n",
    "for i in range(no_users):\n",
    "    for j in range(no_users):\n",
    "        if i != j:  # No self-loops\n",
    "            # Check if users i and j share any modalities\n",
    "            if set(user_modalities[i]) & set(user_modalities[j]):\n",
    "                similarity_matrix[i, j] = 1\n",
    "\n",
    "# Display the adjacency matrix\n",
    "print(\"Adjacency Matrix:\")\n",
    "print(similarity_matrix)\n",
    "\n",
    "# Prepare mixing matrices for each modality\n",
    "#mixing_matrices, adjacency_matrices = prepare_mixing_matrices(adjacency_matrix, similarity_matrices)\n",
    "adjacency_matrix = create_random_topology(20, similarity_matrix, edge_probability=0.3)\n",
    "print(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3c612d-004b-499c-b98a-a5d0cef8cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the graph\n",
    "# Define colors for the groups\n",
    "group_colors = {1: 'red', 2: 'green', 3: 'blue'}\n",
    "node_colors = [group_colors[group] for group in user_groups]\n",
    "G = nx.from_numpy_array(similarity_matrix)\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, edge_color='gray', node_size=1000, node_color=node_colors, font_size=20, font_color='black')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a429b-e190-44c7-b192-4546fac4aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the graph\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "G = nx.from_numpy_array(adjacency_matrix)\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, edge_color='gray', node_size=1000, node_color=node_colors, font_size=20, font_color='black')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "print(user_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d13f9-412a-4aff-a6f9-b5874d4e720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity matrices\n",
    "adj_per_modality = {}\n",
    "for modality in available_modalities:\n",
    "    adj = np.zeros((no_users, no_users))\n",
    "    for node in range(no_users):\n",
    "        for neighbor in G.neighbors(node):\n",
    "            if modality in user_modalities[neighbor] and modality in user_modalities[node]:\n",
    "                adj[node, neighbor] = 1.    \n",
    "    adj_per_modality[modality] = adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a777c2e2-57fe-4b72-9203-b2ff52032787",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_modality = nx.from_numpy_array(adj_per_modality[\"images\"])\n",
    "pos = nx.spring_layout(G_modality)\n",
    "nx.draw(G_modality, pos, with_labels=True, edge_color='gray', node_size=1000, node_color=node_colors, font_size=20, font_color='black')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0d52df-5b53-4854-b693-aa3c55bc3e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_mixing_matrix(Adj, method=\"metropolis\"):\n",
    "    n = Adj.shape[0]\n",
    "    W = np.zeros((n, n))  # Initialize weight matrix\n",
    "\n",
    "    for i in range(n):\n",
    "        degree_i = np.sum(Adj[i, :])\n",
    "\n",
    "        for j in range(n):\n",
    "            if Adj[i, j] == 1.0:\n",
    "                degree_j = np.sum(Adj[j, :])\n",
    "    \n",
    "                if method == \"metropolis\":\n",
    "                    W[i, j] = 1 / (max(degree_i, degree_j) + 1)\n",
    "                elif method == \"uniform\":\n",
    "                    W[i, j] = 1 / degree_i\n",
    "\n",
    "        # Diagonal weight\n",
    "        W[i, i] = 1 - np.sum(W[i, :])\n",
    "\n",
    "    return W\n",
    "\n",
    "mixing_matrices = {}\n",
    "for modality in available_modalities:\n",
    "    mixing_matrices[modality] = construct_mixing_matrix(adj_per_modality[modality], method=\"metropolis\")\n",
    "    print(np.sum(mixing_matrices[modality], 0))\n",
    "    print(np.sum(mixing_matrices[modality], 1))\n",
    "    lamb = np.linalg.eigvals(mixing_matrices[modality])\n",
    "    lamb.sort()\n",
    "    print(lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f06211-192b-4932-a918-b9fe5407bd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_modality = nx.from_numpy_array(adj_per_modality[\"pos_height\"])\n",
    "pos = nx.spring_layout(G_modality)\n",
    "largest_cc = max(nx.connected_components(G_modality), key=len)\n",
    "\n",
    "# Convert to sorted list of indices\n",
    "connected_nodes = sorted(largest_cc)\n",
    "\n",
    "# Extract the submatrix corresponding to the connected subgraph\n",
    "W_reduced = mixing_matrices[\"pos_height\"][np.ix_(connected_nodes, connected_nodes)]\n",
    "lamb = np.linalg.eigvals(W_reduced)\n",
    "lamb.sort()\n",
    "print(lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e64903e-aade-4ec3-b367-87a0a339a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Network\n",
    "class ImageNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageNet, self).__init__()\n",
    "        self.feature_extractor = resnet50(pretrained=True, progress=True, num_classes=64)  # ResNet50 for image input\n",
    "        self.bn = nn.BatchNorm1d(128)  # Batch Normalization for image stream\n",
    "        self.fc = MultinomialLogisticRegression(input_size=128, num_classes=64)  # Adjust input_size & num_classes\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Position Network\n",
    "class PosNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PosNet, self).__init__()\n",
    "        self.feature_extractor = NN_beam_pred(num_features=4, num_output=64)  # Neural network for position input\n",
    "        self.bn = nn.BatchNorm1d(128)  # Batch Normalization for position stream\n",
    "        self.fc = MultinomialLogisticRegression(input_size=128, num_classes=64)  # Adjust input_size & num_classes\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca511b8-585b-4698-a6bc-bfb3ab69ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "optimizers = []\n",
    "for user_id in range(no_users):\n",
    "    user_model = {}\n",
    "    local_optimizer = {}\n",
    "    if \"images\" in user_modalities[user_id]:\n",
    "        user_model[\"images\"] = ImageNet().to(device)\n",
    "        #user_model[\"images\"] = resnet50(pretrained=True, progress=True, num_classes=64)\n",
    "        local_optimizer[\"images\"] = optim.Adam(user_model[\"images\"].parameters(), lr=lr)\n",
    "    if \"pos_height\" in user_modalities[user_id]:\n",
    "        user_model[\"pos_height\"] = PosNet().to(device)\n",
    "        #user_model[\"pos_height\"] = NN_beam_pred(num_features=4, num_output=64)\n",
    "        local_optimizer[\"pos_height\"] = optim.Adam(user_model[\"pos_height\"].parameters(), lr=lr)\n",
    "    all_models.append(user_model)\n",
    "    optimizers.append(local_optimizer)\n",
    "base_models = {\"images\": ImageNet().to(device), \"pos_height\": PosNet().to(device)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ccd1e-5893-492e-aaf8-7406a37793fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Decentralized aggregation function\n",
    "def decentralized_aggregation(user_models, mixing_matrices, available_modalities):\n",
    "    num_users = len(user_models)\n",
    "    \n",
    "    for modality in available_modalities:\n",
    "        # Get the mixing matrix for the current modality\n",
    "        mixing_matrix = mixing_matrices[modality]\n",
    "        \n",
    "        # Convert user model parameters to vectors for aggregation\n",
    "        aggregated_models = []\n",
    "        aggregated_updates = []\n",
    "        for user_model in user_models:\n",
    "            if modality in user_model.keys(): \n",
    "                aggregated_models.append(torch.nn.utils.parameters_to_vector(user_model[modality].parameters()))\n",
    "                aggregated_updates.append(torch.zeros_like(aggregated_models[-1]))\n",
    "            else:\n",
    "                aggregated_models.append(0)\n",
    "                aggregated_updates.append(0)\n",
    "        \n",
    "        \n",
    "        # Perform model aggregation based on the mixing matrix for this modality\n",
    "        for i in range(num_users):\n",
    "            for j in range(num_users):\n",
    "                if mixing_matrix[i, j] > 0:\n",
    "                    aggregated_updates[i] += mixing_matrix[i, j] * aggregated_models[j]\n",
    "        \n",
    "        # Update user models with aggregated parameters for the current modality\n",
    "        for i in range(num_users):\n",
    "            if modality in user_models[i].keys():\n",
    "                torch.nn.utils.vector_to_parameters(aggregated_updates[i], user_models[i][modality].parameters())\n",
    "\n",
    "\n",
    "def train_local_model(local_modalities, models, train_loader, criterion, optimizers, epochs):\n",
    "\n",
    "    training_losses = []\n",
    "    training_accuracies = []\n",
    "\n",
    "    max_acc = 0\n",
    "    max_loss = 0\n",
    "\n",
    "    for modality in local_modalities:\n",
    "        #print(f\"Training for modality: {modality}\")\n",
    "\n",
    "        model = models[modality]\n",
    "        optimizer = optimizers[modality]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()  # Set model to training mode\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            correct_predictions = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            for data_batch, labels in train_loader:\n",
    "                # Get the data for the current modality\n",
    "                data = data_batch[modality]\n",
    "                \n",
    "\n",
    "                # Move data to GPU if available\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(data)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update loss and accuracy metrics\n",
    "                epoch_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "            # Calculate average loss and accuracy for the epoch\n",
    "            avg_loss = epoch_loss / len(train_loader)\n",
    "            accuracy = correct_predictions / total_samples\n",
    "\n",
    "            # Store metrics for debugging\n",
    "            training_losses.append(avg_loss)\n",
    "            training_accuracies.append(accuracy)\n",
    "\n",
    "            #print(\n",
    "            #    f\"Epoch [{epoch + 1}/{epochs}], Modality: {modality}, \"\n",
    "            #    f\"Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\"\n",
    "            #)\n",
    "\n",
    "    return min(training_losses), max(training_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283f587-3182-4141-9c59-7123ee545153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_user_models(user_id, user_models, val_loaders, criterion):\n",
    "\n",
    "    total_acc = []\n",
    "    total_los = []\n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        for modality, model in user_models.items():\n",
    "            total_loss = 0.0\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "            model.eval()\n",
    "            if modality not in user_models.keys():\n",
    "                print(f\"Skipping modality {modality} for User {user_id + 1}, no validation data.\")\n",
    "                continue\n",
    "            \n",
    "            for data, labels in val_loaders:  # Iterate over validation data for the modality\n",
    "                data = data[modality]\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Accumulate loss and accuracy\n",
    "                total_loss += loss.item() * labels.size(0)  # Sum loss for the batch\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_correct += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "            # Compute metrics\n",
    "            avg_loss = total_loss / total_samples if total_samples > 0 else 0.0\n",
    "            accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
    "            total_acc.append(accuracy)\n",
    "            total_los.append(avg_loss)\n",
    "        \n",
    "            #print(f\"User {user_id + 1}, modality: {modality} - Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        return {'loss': min(total_los), 'accuracy': max(total_acc)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c9171-1ac2-49ec-bb29-eecec3a3c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lambda_reg = 0.01\n",
    "eta = 0.01\n",
    "\n",
    "# Dictionaries to store metrics\n",
    "group_train_loss_histories = {1: [], 2: [], 3: []}\n",
    "group_train_accuracy_histories = {1: [], 2: [], 3: []}\n",
    "group_val_loss_histories = {1: [], 2: [], 3: []}\n",
    "group_val_accuracy_histories = {1: [], 2: [], 3: []}\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "global_rounds = 100\n",
    "local_epochs = 1\n",
    "# Decentralized Federated Learning Loop\n",
    "for round_num in range(global_rounds):\n",
    "\n",
    "    # Decentralized aggregation\n",
    "    #decentralized_aggregation(all_models, mixing_matrices, available_modalities)\n",
    "    epoch_group_train_losses = {1: [], 2: [], 3: []}\n",
    "    epoch_group_train_accuracies = {1: [], 2: [], 3: []}\n",
    "    epoch_group_val_losses = {1: [], 2: [], 3: []}\n",
    "    epoch_group_val_accuracies = {1: [], 2: [], 3: []}\n",
    "\n",
    "    \n",
    "    print(f\"Global Round {round_num + 1}\")\n",
    "\n",
    "    # Training for image modalities\n",
    "    \n",
    "    epoch_train_loss = 0.0\n",
    "    correct_train_predictions = 0\n",
    "    total_train_samples = 0\n",
    "\n",
    "    for user_id in range(no_users):\n",
    "        #print(f\"Training model for User {user_id + 1}\")\n",
    "        print(user_id)\n",
    "        user_models = all_models[user_id]\n",
    "        group = user_groups[user_id]\n",
    "        \n",
    "        # Initialize optimizers for each modality\n",
    "        \n",
    "        # Train local model\n",
    "        train_losses, train_accuracies = train_local_model(\n",
    "            user_modalities[user_id],\n",
    "            user_models,\n",
    "            train_loaders[user_id],\n",
    "            criterion,\n",
    "            optimizers[user_id],\n",
    "            local_epochs\n",
    "        )\n",
    "        epoch_group_train_losses[group].append(train_losses)\n",
    "        epoch_group_train_accuracies[group].append(train_accuracies)\n",
    "\n",
    "        # Assign metrics to the respective group\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    # Validation phase\n",
    "    \n",
    "    for user_id in range(no_users):\n",
    "        user_models = all_models[user_id]\n",
    "        val_dict = validate_user_models(\n",
    "            user_id,\n",
    "            user_models,\n",
    "            val_loaders[user_id],\n",
    "            criterion\n",
    "        )\n",
    "        group = user_groups[user_id]\n",
    "        epoch_group_val_losses[group].append(val_dict[\"loss\"])\n",
    "        epoch_group_val_accuracies[group].append(val_dict[\"accuracy\"])\n",
    "\n",
    "    # Print group-wise metrics\n",
    "    #print(f\"Metrics for Global Round {round_num + 1}:\")\n",
    "    for group in [1, 2, 3]:\n",
    "\n",
    "        group_train_loss_histories[group].append(epoch_group_train_losses[group])\n",
    "        group_train_accuracy_histories[group].append(epoch_group_train_accuracies[group])\n",
    "        group_val_loss_histories[group].append(epoch_group_val_losses[group])\n",
    "        group_val_accuracy_histories[group].append(epoch_group_val_accuracies[group])\n",
    "\n",
    "    print(f\"{global_rounds}] Group Metrics:\")\n",
    "    for group in [1, 2, 3]:\n",
    "        print(f\"  Group {group} - Train Loss: {np.mean(group_train_loss_histories[group][-1]):.4f}, Train Accuracy: {np.mean(group_train_accuracy_histories[group][-1]):.4f}\")\n",
    "        print(f\"  Group {group} - Val Loss: {np.mean(group_val_loss_histories[group][-1]):.4f}, Val Accuracy: {np.mean(group_val_accuracy_histories[group][-1]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f5b0d-19b9-4050-b245-6d13078d0066",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class MultiStreamNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiStreamNet, self).__init__()\n",
    "\n",
    "        # Define the streams\n",
    "        self.stream_1 = resnet50(pretrained=True, progress=True, num_classes=64)  # First stream using resnet50\n",
    "        self.stream_2 = NN_beam_pred(num_features=4, num_output=64)  # Second stream using NN_beam_pred\n",
    "        \n",
    "        # Add Batch Normalization layers\n",
    "        self.bn_stream_1 = nn.BatchNorm1d(128)  # Adjust according to the output size of resnet50\n",
    "        self.bn_stream_2 = nn.BatchNorm1d(128)   # Adjust according to the output size of NN_beam_pred\n",
    "\n",
    "        # Combine the two streams and apply Multinomial Logistic Regression\n",
    "        self.fc = MultinomialLogisticRegression(input_size=256, num_classes=64)  # Adjust input_size & num_classes\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Process through stream 1 (resnet50)\n",
    "        out1 = self.stream_1(x1)\n",
    "        out2 = self.bn_stream_1(out1)\n",
    "        print(out1.shape)\n",
    "        print(out2.shape)\n",
    "        \n",
    "        # Process through stream 2 (NN_beam_pred)\n",
    "        out2 = self.stream_2(x2)\n",
    "        out2 = self.bn_stream_2(out2)\n",
    "\n",
    "        # Concatenate the outputs from both streams\n",
    "        out = torch.cat((out1, out2), dim=1)\n",
    "        \n",
    "        # Final classification layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Instantiate the model\n",
    "model = MultiStreamNet().to(device)\n",
    "\n",
    "# Set the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Example training loop\n",
    "def train(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            x, y = data  # Assuming inputs1 and inputs2 are from different streams\n",
    "            x[\"images\"], x[\"pos_height\"], y = x[\"images\"].to(device), x[\"pos_height\"].to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(x[\"images\"], x[\"pos_height\"])\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, y)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Print average loss per epoch\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Assuming `train_loader` is a DataLoader object containing the training data\n",
    "train(model, train_loaders[0], criterion, optimizer)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6e4d18-b3ff-4a6f-b813-e1db3738d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = global_rounds\n",
    "# Convert metrics to numpy arrays for easy manipulation\n",
    "group_train_loss_histories = {k: np.array(v) for k, v in group_train_loss_histories.items()}\n",
    "group_train_accuracy_histories = {k: np.array(v) for k, v in group_train_accuracy_histories.items()}\n",
    "group_val_loss_histories = {k: np.array(v) for k, v in group_val_loss_histories.items()}\n",
    "group_val_accuracy_histories = {k: np.array(v) for k, v in group_val_accuracy_histories.items()}\n",
    "\n",
    "# Handle potential one-dimensional arrays\n",
    "group_train_loss_mean = {k: v.mean(axis=1) if v.ndim > 1 else v for k, v in group_train_loss_histories.items()}\n",
    "group_train_loss_std = {k: v.std(axis=1) if v.ndim > 1 else np.zeros_like(v) for k, v in group_train_loss_histories.items()}\n",
    "group_val_loss_mean = {k: v.mean(axis=1) if v.ndim > 1 else v for k, v in group_val_loss_histories.items()}\n",
    "group_val_loss_std = {k: v.std(axis=1) if v.ndim > 1 else np.zeros_like(v) for k, v in group_val_loss_histories.items()}\n",
    "\n",
    "group_train_acc_mean = {k: v.mean(axis=1) if v.ndim > 1 else v for k, v in group_train_accuracy_histories.items()}\n",
    "group_train_acc_std = {k: v.std(axis=1) if v.ndim > 1 else np.zeros_like(v) for k, v in group_train_accuracy_histories.items()}\n",
    "group_val_acc_mean = {k: v.mean(axis=1) if v.ndim > 1 else v for k, v in group_val_accuracy_histories.items()}\n",
    "group_val_acc_std = {k: v.std(axis=1) if v.ndim > 1 else np.zeros_like(v) for k, v in group_val_accuracy_histories.items()}\n",
    "\n",
    "# Combined Plot for All Modalities\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "for group in [1, 2, 3]:\n",
    "    plt.plot(range(1, num_epochs + 1), group_train_loss_mean[group], label=f\"Group {group} Train Loss\")\n",
    "    plt.fill_between(range(1, num_epochs + 1), \n",
    "                     group_train_loss_mean[group] - group_train_loss_std[group], \n",
    "                     group_train_loss_mean[group] + group_train_loss_std[group], \n",
    "                     alpha=0.2)\n",
    "    plt.plot(range(1, num_epochs + 1), group_val_loss_mean[group], label=f\"Group {group} Validation Loss\", linestyle=\"dashed\")\n",
    "    plt.fill_between(range(1, num_epochs + 1), \n",
    "                     group_val_loss_mean[group] - group_val_loss_std[group], \n",
    "                     group_val_loss_mean[group] + group_val_loss_std[group], \n",
    "                     alpha=0.2)\n",
    "\n",
    "plt.title(\"Loss over Epochs for All Groups\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Accuracy Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "for group in [1, 2, 3]:\n",
    "    plt.plot(range(1, num_epochs + 1), group_train_acc_mean[group], label=f\"Group {group} Train Accuracy\")\n",
    "    plt.fill_between(range(1, num_epochs + 1), \n",
    "                     group_train_acc_mean[group] - group_train_acc_std[group], \n",
    "                     group_train_acc_mean[group] + group_train_acc_std[group], \n",
    "                     alpha=0.2)\n",
    "    plt.plot(range(1, num_epochs + 1), group_val_acc_mean[group], label=f\"Group {group} Validation Accuracy\", linestyle=\"dashed\")\n",
    "    plt.fill_between(range(1, num_epochs + 1), \n",
    "                     group_val_acc_mean[group] - group_val_acc_std[group], \n",
    "                     group_val_acc_mean[group] + group_val_acc_std[group], \n",
    "                     alpha=0.2)\n",
    "\n",
    "plt.title(\"Accuracy over Epochs for All Groups\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([0, 1])  # Ensure y-axis is between 0 and 1 for accuracy\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef6bc25-66fc-463b-bf3b-649b95dad7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert numpy arrays to lists for serialization\n",
    "data_to_save = {\n",
    "    \"group_train_loss_mean\": {k: v.tolist() for k, v in group_train_loss_mean.items()},\n",
    "    \"group_train_loss_std\": {k: v.tolist() for k, v in group_train_loss_std.items()},\n",
    "    \"group_val_loss_mean\": {k: v.tolist() for k, v in group_val_loss_mean.items()},\n",
    "    \"group_val_loss_std\": {k: v.tolist() for k, v in group_val_loss_std.items()},\n",
    "    \"group_train_acc_mean\": {k: v.tolist() for k, v in group_train_acc_mean.items()},\n",
    "    \"group_train_acc_std\": {k: v.tolist() for k, v in group_train_acc_std.items()},\n",
    "    \"group_val_acc_mean\": {k: v.tolist() for k, v in group_val_acc_mean.items()},\n",
    "    \"group_val_acc_std\": {k: v.tolist() for k, v in group_val_acc_std.items()}\n",
    "}\n",
    "\n",
    "with open(\"local_drone.json\", \"w\") as f:\n",
    "    json.dump(data_to_save, f)\n",
    "print(\"DSGD_metrics.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
