{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14b537c0-5308-494e-b479-df1ed1fd34b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transf\n",
    "from data_feed import DataFeed, DataFeed_image_pos\n",
    "from build_net import resnet50, NN_beam_pred, MultinomialLogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aa17e9f-2958-489b-af3e-4dab1dfe1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# or full reproducibility\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38bfa9a-c6ef-450a-9180-61eb294a2ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Now you can use the `device` variable to move your model and data to the correct device\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be724f96-0c63-47c8-ad68-1b617c05d244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Overview:\n",
      "   index_x  unit2_lat  unit2_lon  unit2_height  unit2_distance  \\\n",
      "0     9862   0.188929   0.431909      1.000000        0.167641   \n",
      "1     4449   0.410610   0.286188      0.231752        0.361212   \n",
      "2    10299   0.365071   0.424029      0.174270        0.246799   \n",
      "3     9229   0.335884   0.431909      0.656934        0.210932   \n",
      "4     6947   0.175857   0.390571      0.056569        0.242568   \n",
      "\n",
      "                                           unit1_rgb  unit1_beam_x  \\\n",
      "0  ./../Carla/datasets/scenario23_dev//unit1/came...            17   \n",
      "1  ./../Carla/datasets/scenario23_dev//unit1/came...            14   \n",
      "2  ./../Carla/datasets/scenario23_dev//unit1/came...            20   \n",
      "3  ./../Carla/datasets/scenario23_dev//unit1/came...            17   \n",
      "4  ./../Carla/datasets/scenario23_dev//unit1/came...            14   \n",
      "\n",
      "   lat_region  lon_region  region  \n",
      "0           0           1       1  \n",
      "1           1           0       2  \n",
      "2           1           1       3  \n",
      "3           1           1       3  \n",
      "4           0           0       0  \n",
      "\n",
      "Data Summary:\n",
      "            index_x   unit2_lat   unit2_lon  unit2_height  unit2_distance  \\\n",
      "count    570.000000  570.000000  570.000000    570.000000      570.000000   \n",
      "mean    5765.140351    0.339799    0.404146      0.326157        0.276529   \n",
      "std     3235.268614    0.154570    0.105268      0.267289        0.152336   \n",
      "min       20.000000    0.030779    0.081985      0.017336        0.001901   \n",
      "25%     3018.500000    0.226024    0.352309      0.092153        0.173065   \n",
      "50%     5894.500000    0.306971    0.412554      0.208029        0.231712   \n",
      "75%     8530.500000    0.399213    0.451542      0.552007        0.357941   \n",
      "max    11349.000000    0.981461    0.845154      1.000000        0.920808   \n",
      "\n",
      "       unit1_beam_x  lat_region  lon_region      region  \n",
      "count    570.000000  570.000000  570.000000  570.000000  \n",
      "mean      15.608772    0.492982    0.524561    1.510526  \n",
      "std        4.621672    0.500390    0.499835    1.181608  \n",
      "min        2.000000    0.000000    0.000000    0.000000  \n",
      "25%       14.000000    0.000000    0.000000    0.000000  \n",
      "50%       16.000000    0.000000    1.000000    1.000000  \n",
      "75%       17.000000    1.000000    1.000000    3.000000  \n",
      "max       30.000000    1.000000    1.000000    3.000000  \n",
      "\n",
      "Missing Values:\n",
      "index_x           0\n",
      "unit2_lat         0\n",
      "unit2_lon         0\n",
      "unit2_height      0\n",
      "unit2_distance    0\n",
      "unit1_rgb         0\n",
      "unit1_beam_x      0\n",
      "lat_region        0\n",
      "lon_region        0\n",
      "region            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the saved CSV files\n",
    "output_dir = \"./feature_IID/\"\n",
    "\n",
    "# Load one of the CSV files for EDA (e.g., user_0_outputs.csv)\n",
    "df = pd.read_csv(output_dir + \"user_0_pos_height_beam.csv\")\n",
    "\n",
    "# Quick overview of the data\n",
    "print(\"Data Overview:\")\n",
    "print(df.head())\n",
    "print(\"\\nData Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43bf9b52-16e1-4ec8-acd5-1cbd19cab71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All loadred are loaded\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "########################### Data pre-processing ########################\n",
    "########################################################################\n",
    "from torch.utils.data import ConcatDataset\n",
    "no_users = 1\n",
    "all_datasets_splits = 20\n",
    "batch_size = 64\n",
    "img_resize = transf.Resize((224, 224))\n",
    "img_norm = transf.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "proc_pipe = transf.Compose(\n",
    "    [transf.ToPILImage(),\n",
    "     img_resize,\n",
    "     transf.ToTensor(),\n",
    "     img_norm]\n",
    ")\n",
    "dataset_dir = \"feature_IID/\"\n",
    "train_loaders = []\n",
    "test_loaders = []\n",
    "val_loaders = []\n",
    "train_combined_dataset = []\n",
    "val_combined_datasets = []\n",
    "for user_id in range(all_datasets_splits):\n",
    "    train_dir = dataset_dir + f'user_{user_id}_pos_height_beam_train.csv'\n",
    "    val_dir = dataset_dir + f'user_{user_id}_pos_height_beam_val.csv'\n",
    "    test_dir = dataset_dir + f'user_{user_id}_pos_height_beam_test.csv'\n",
    "    \n",
    "    train_dataset = DataFeed_image_pos(train_dir, transform=proc_pipe)\n",
    "    val_dataset = DataFeed_image_pos(root_dir=val_dir, transform=proc_pipe)\n",
    "    test_dataset = DataFeed_image_pos(root_dir=test_dir, transform=proc_pipe)\n",
    "    train_combined_dataset.append(train_dataset)\n",
    "    val_combined_datasets.append(val_dataset)\n",
    "\n",
    "train_dataset = ConcatDataset(train_combined_dataset)\n",
    "val_dataset = ConcatDataset(val_combined_datasets)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              #num_workers=8,\n",
    "                              shuffle=False)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        #num_workers=8,\n",
    "                        shuffle=False)\n",
    "\n",
    "print(\"All loadred are loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf2aeb40-49ab-4536-a119-acf212a41a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Preperation#\n",
    "all_models = []\n",
    "available_modalities = [\"pos_height\", \"images\"]\n",
    "modality_size = {\"pos_height\": 128, \"images\": 128}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e5627ec-2bc5-406a-bed8-a058e4a728b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Groups: [1, 2, 3]\n",
      "User Modalities: [['pos_height'], ['images'], ['pos_height', 'images']]\n",
      "Output Sizes: [128, 128, 256]\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "import random\n",
    "no_users = 3  # Example: Number of users\n",
    "available_modalities = [\"pos_height\", \"images\"]\n",
    "modality_size = {\"pos_height\": 128, \"images\": 128}\n",
    "group_definitions = {\n",
    "    1: [\"pos_height\"],        # Group 1: Only pos_height\n",
    "    2: [\"images\"],            # Group 2: Only images\n",
    "    3: [\"pos_height\", \"images\"]  # Group 3: Both modalities\n",
    "}\n",
    "\n",
    "# Assign each user to a group randomly\n",
    "weights = [0.2, 0.3, 0.5]  # Probabilities for groups 1, 2, and 3\n",
    "\n",
    "# Generate user_groups with weighted random choices\n",
    "user_groups = [1, 2, 3] #random.choices([1, 2, 3], weights=weights, k=no_users)\n",
    "\n",
    "# Assign modalities to users based on their group\n",
    "user_modalities = [group_definitions[group] for group in user_groups]\n",
    "\n",
    "# Compute output sizes for each user based on their modalities\n",
    "output_sizes = [sum(modality_size[modality] for modality in user_modality) for user_modality in user_modalities]\n",
    "\n",
    "# Store models (placeholders for actual models)\n",
    "all_models = []\n",
    "\n",
    "# Example output (for verification)\n",
    "print(f\"User Groups: {user_groups[:10]}\")  # Show first 10 users' groups\n",
    "print(f\"User Modalities: {user_modalities[:10]}\")  # Show first 10 users' modalities\n",
    "print(f\"Output Sizes: {output_sizes[:10]}\")  # Show first 10 users' output sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0ed4d5c-f244-40b5-9036-25a31edc6e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_knopp(matrix, tol=1e-9, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Converts a given matrix to a doubly stochastic matrix using the Sinkhorn-Knopp algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "        matrix (np.ndarray): The input matrix to be transformed.\n",
    "        tol (float): The tolerance for convergence.\n",
    "        max_iter (int): Maximum number of iterations for convergence.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A doubly stochastic matrix.\n",
    "    \"\"\"\n",
    "    matrix = matrix.copy()\n",
    "    for _ in range(max_iter):\n",
    "        # Normalize rows\n",
    "        row_sums = matrix.sum(axis=1, keepdims=True)\n",
    "        matrix /= row_sums\n",
    "\n",
    "        # Normalize columns\n",
    "        col_sums = matrix.sum(axis=0, keepdims=True)\n",
    "        matrix /= col_sums\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.allclose(matrix.sum(axis=1), 1, atol=tol) and np.allclose(matrix.sum(axis=0), 1, atol=tol):\n",
    "            break\n",
    "\n",
    "    return matrix\n",
    "    \n",
    "def create_random_topology(num_users, similarity_matrix, edge_probability=0.3):\n",
    "    \"\"\"\n",
    "    Creates a connected random topology using NetworkX.\n",
    "    Returns the adjacency matrix.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        graph = nx.erdos_renyi_graph(num_users, edge_probability)\n",
    "        adjacency_matrix = nx.to_numpy_array(graph)\n",
    "        new_adj = np.multiply(adjacency_matrix, similarity_matrix)\n",
    "        new_graph = nx.from_numpy_array(new_adj)\n",
    "        if nx.is_connected(new_graph):\n",
    "            break\n",
    "\n",
    "    # Convert graph to adjacency matrix\n",
    "    adjacency_matrix = nx.to_numpy_array(new_graph)\n",
    "    return adjacency_matrix\n",
    "\n",
    "def prepare_mixing_matrices(adjacency_matrix, similarity_matrices):\n",
    "    \"\"\"\n",
    "    Computes a mixing matrix for each modality by multiplying the adjacency matrix \n",
    "    with the similarity matrix for that modality.\n",
    "    Returns a dictionary of mixing matrices.\n",
    "    \"\"\"\n",
    "    adjacency_matrices = {}\n",
    "    mixing_matrices = {}\n",
    "    for modality, similarity_matrix in similarity_matrices.items():\n",
    "        # Element-wise multiplication of adjacency and similarity matrices\n",
    "        combined_matrix = adjacency_matrix * similarity_matrix\n",
    "        adjacency_matrices[modality] = combined_matrix\n",
    "        \n",
    "        # Normalize to create a doubly matrix\n",
    "        mixing_matrix = sinkhorn_knopp(combined_matrix)\n",
    "        \n",
    "        \n",
    "        mixing_matrices[modality] = mixing_matrix\n",
    "    \n",
    "    return mixing_matrices, adjacency_matrices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a3c612d-004b-499c-b98a-a5d0cef8cee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Draw the graph\n",
    "# Define colors for the groups\n",
    "group_colors = {1: 'red', 2: 'green', 3: 'blue'}\n",
    "node_colors = [group_colors[group] for group in user_groups]\n",
    "G = nx.from_numpy_array(similarity_matrix)\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, edge_color='gray', node_size=1000, node_color=node_colors, font_size=20, font_color='black')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e64903e-aade-4ec3-b367-87a0a339a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Network\n",
    "class ImageNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageNet, self).__init__()\n",
    "        self.feature_extractor = resnet50(pretrained=True, progress=True, num_classes=64)  # ResNet50 for image input\n",
    "        self.bn = nn.BatchNorm1d(128)  # Batch Normalization for image stream\n",
    "        self.fc = MultinomialLogisticRegression(input_size=128, num_classes=64)  # Adjust input_size & num_classes\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Position Network\n",
    "class PosNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PosNet, self).__init__()\n",
    "        self.feature_extractor = NN_beam_pred(num_features=4, num_output=64)  # Neural network for position input\n",
    "        self.bn = nn.BatchNorm1d(128)  # Batch Normalization for position stream\n",
    "        self.fc = MultinomialLogisticRegression(input_size=128, num_classes=64)  # Adjust input_size & num_classes\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ca511b8-585b-4698-a6bc-bfb3ab69ea83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n",
      "Output layer dim = 64\n",
      "<class 'build_net.Bottleneck'>\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "optimizers = []\n",
    "for user_id in range(no_users):\n",
    "    user_model = {}\n",
    "    local_optimizer = {}\n",
    "    if \"images\" in user_modalities[user_id]:\n",
    "        user_model[\"images\"] = ImageNet().to(device)\n",
    "        #user_model[\"images\"] = resnet50(pretrained=True, progress=True, num_classes=64)\n",
    "        local_optimizer[\"images\"] = optim.Adam(user_model[\"images\"].parameters(), lr=lr)\n",
    "    if \"pos_height\" in user_modalities[user_id]:\n",
    "        user_model[\"pos_height\"] = PosNet().to(device)\n",
    "        #user_model[\"pos_height\"] = NN_beam_pred(num_features=4, num_output=64)\n",
    "        local_optimizer[\"pos_height\"] = optim.Adam(user_model[\"pos_height\"].parameters(), lr=lr)\n",
    "    all_models.append(user_model)\n",
    "    optimizers.append(local_optimizer)\n",
    "base_models = {\"images\": ImageNet().to(device), \"pos_height\": PosNet().to(device)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f56ccd1e-5893-492e-aaf8-7406a37793fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Decentralized aggregation function\n",
    "def decentralized_aggregation(user_models, mixing_matrices, available_modalities):\n",
    "    num_users = len(user_models)\n",
    "    \n",
    "    for modality in available_modalities:\n",
    "        # Get the mixing matrix for the current modality\n",
    "        mixing_matrix = mixing_matrices[modality]\n",
    "        \n",
    "        # Convert user model parameters to vectors for aggregation\n",
    "        aggregated_models = []\n",
    "        aggregated_updates = []\n",
    "        for user_model in user_models:\n",
    "            if modality in user_model.keys(): \n",
    "                aggregated_models.append(torch.nn.utils.parameters_to_vector(user_model[modality].parameters()))\n",
    "                aggregated_updates.append(torch.zeros_like(aggregated_models[-1]))\n",
    "            else:\n",
    "                aggregated_models.append(0)\n",
    "                aggregated_updates.append(0)\n",
    "        \n",
    "        \n",
    "        # Perform model aggregation based on the mixing matrix for this modality\n",
    "        for i in range(num_users):\n",
    "            for j in range(num_users):\n",
    "                if mixing_matrix[i, j] > 0:\n",
    "                    aggregated_updates[i] += mixing_matrix[i, j] * aggregated_models[j]\n",
    "        \n",
    "        # Update user models with aggregated parameters for the current modality\n",
    "        for i in range(num_users):\n",
    "            if modality in user_models[i].keys():\n",
    "                torch.nn.utils.vector_to_parameters(aggregated_updates[i], user_models[i][modality].parameters())\n",
    "\n",
    "\n",
    "def train_local_model(local_modalities, models, train_loader, criterion, optimizers, epochs):\n",
    "\n",
    "    training_losses = []\n",
    "    training_accuracies = []\n",
    "\n",
    "    max_acc = 0\n",
    "    max_loss = 0\n",
    "\n",
    "    for modality in local_modalities:\n",
    "        #print(f\"Training for modality: {modality}\")\n",
    "\n",
    "        model = models[modality]\n",
    "        optimizer = optimizers[modality]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()  # Set model to training mode\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            correct_predictions = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            for data_batch, labels in train_loader:\n",
    "                # Get the data for the current modality\n",
    "                data = data_batch[modality]\n",
    "                \n",
    "\n",
    "                # Move data to GPU if available\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(data)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update loss and accuracy metrics\n",
    "                epoch_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "            # Calculate average loss and accuracy for the epoch\n",
    "            avg_loss = epoch_loss / len(train_loader)\n",
    "            accuracy = correct_predictions / total_samples\n",
    "\n",
    "            # Store metrics for debugging\n",
    "            training_losses.append(avg_loss)\n",
    "            training_accuracies.append(accuracy)\n",
    "\n",
    "            #print(\n",
    "            #    f\"Epoch [{epoch + 1}/{epochs}], Modality: {modality}, \"\n",
    "            #    f\"Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\"\n",
    "            #)\n",
    "\n",
    "    return min(training_losses), max(training_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1283f587-3182-4141-9c59-7123ee545153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_user_models(user_id, user_models, val_loaders, criterion):\n",
    "\n",
    "    total_acc = []\n",
    "    total_los = []\n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        for modality, model in user_models.items():\n",
    "            total_loss = 0.0\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "            model.eval()\n",
    "            if modality not in user_models.keys():\n",
    "                print(f\"Skipping modality {modality} for User {user_id + 1}, no validation data.\")\n",
    "                continue\n",
    "            \n",
    "            for data, labels in val_loaders:  # Iterate over validation data for the modality\n",
    "                data = data[modality]\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Accumulate loss and accuracy\n",
    "                total_loss += loss.item() * labels.size(0)  # Sum loss for the batch\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_correct += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "            # Compute metrics\n",
    "            avg_loss = total_loss / total_samples if total_samples > 0 else 0.0\n",
    "            accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
    "            total_acc.append(accuracy)\n",
    "            total_los.append(avg_loss)\n",
    "        \n",
    "            #print(f\"User {user_id + 1}, modality: {modality} - Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        return {'loss': min(total_los), 'accuracy': max(total_acc)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb1c9171-1ac2-49ec-bb29-eecec3a3c260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Round 1\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 42\u001b[0m\n\u001b[0;32m     37\u001b[0m group \u001b[38;5;241m=\u001b[39m user_groups[user_id]\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Initialize optimizers for each modality\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Train local model\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m train_losses, train_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_local_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_modalities\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_models\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_epochs\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m epoch_group_train_losses[group]\u001b[38;5;241m.\u001b[39mappend(train_losses)\n\u001b[0;32m     51\u001b[0m epoch_group_train_accuracies[group]\u001b[38;5;241m.\u001b[39mappend(train_accuracies)\n",
      "Cell \u001b[1;32mIn[29], line 66\u001b[0m, in \u001b[0;36mtrain_local_model\u001b[1;34m(local_modalities, models, train_loader, criterion, optimizers, epochs)\u001b[0m\n\u001b[0;32m     63\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     69\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[27], line 11\u001b[0m, in \u001b[0;36mImageNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(x)\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Downloads\\Git_Projects\\Multimodal\\build_net.py:220\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    218\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m    219\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m--> 220\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n\u001b[0;32m    223\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "lambda_reg = 0.01\n",
    "eta = 0.01\n",
    "\n",
    "# Dictionaries to store metrics\n",
    "group_train_loss_histories = {1: [], 2: [], 3: []}\n",
    "group_train_accuracy_histories = {1: [], 2: [], 3: []}\n",
    "group_val_loss_histories = {1: [], 2: [], 3: []}\n",
    "group_val_accuracy_histories = {1: [], 2: [], 3: []}\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "global_rounds = 100\n",
    "local_epochs = 1\n",
    "# Decentralized Federated Learning Loop\n",
    "for round_num in range(global_rounds):\n",
    "\n",
    "    # Decentralized aggregation\n",
    "    #decentralized_aggregation(all_models, mixing_matrices, available_modalities)\n",
    "    epoch_group_train_losses = {1: [], 2: [], 3: []}\n",
    "    epoch_group_train_accuracies = {1: [], 2: [], 3: []}\n",
    "    epoch_group_val_losses = {1: [], 2: [], 3: []}\n",
    "    epoch_group_val_accuracies = {1: [], 2: [], 3: []}\n",
    "\n",
    "    \n",
    "    print(f\"Global Round {round_num + 1}\")\n",
    "\n",
    "    # Training for image modalities\n",
    "    \n",
    "    epoch_train_loss = 0.0\n",
    "    correct_train_predictions = 0\n",
    "    total_train_samples = 0\n",
    "\n",
    "    for user_id in range(no_users):\n",
    "        #print(f\"Training model for User {user_id + 1}\")\n",
    "        print(user_id)\n",
    "        user_models = all_models[user_id]\n",
    "        group = user_groups[user_id]\n",
    "        \n",
    "        # Initialize optimizers for each modality\n",
    "        \n",
    "        # Train local model\n",
    "        train_losses, train_accuracies = train_local_model(\n",
    "            user_modalities[user_id],\n",
    "            user_models,\n",
    "            train_loader,\n",
    "            criterion,\n",
    "            optimizers[user_id],\n",
    "            local_epochs\n",
    "        )\n",
    "        epoch_group_train_losses[group].append(train_losses)\n",
    "        epoch_group_train_accuracies[group].append(train_accuracies)\n",
    "\n",
    "        # Assign metrics to the respective group\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    # Validation phase\n",
    "    \n",
    "    for user_id in range(no_users):\n",
    "        user_models = all_models[user_id]\n",
    "        val_dict = validate_user_models(\n",
    "            user_id,\n",
    "            user_models,\n",
    "            val_loader,\n",
    "            criterion\n",
    "        )\n",
    "        group = user_groups[user_id]\n",
    "        epoch_group_val_losses[group].append(val_dict[\"loss\"])\n",
    "        epoch_group_val_accuracies[group].append(val_dict[\"accuracy\"])\n",
    "\n",
    "    # Print group-wise metrics\n",
    "    #print(f\"Metrics for Global Round {round_num + 1}:\")\n",
    "    for group in [1, 2, 3]:\n",
    "\n",
    "        group_train_loss_histories[group].append(epoch_group_train_losses[group])\n",
    "        group_train_accuracy_histories[group].append(epoch_group_train_accuracies[group])\n",
    "        group_val_loss_histories[group].append(epoch_group_val_losses[group])\n",
    "        group_val_accuracy_histories[group].append(epoch_group_val_accuracies[group])\n",
    "\n",
    "    print(f\"{global_rounds}] Group Metrics:\")\n",
    "    for group in [1, 2, 3]:\n",
    "        print(f\"  Group {group} - Train Loss: {np.mean(group_train_loss_histories[group][-1]):.4f}, Train Accuracy: {np.mean(group_train_accuracy_histories[group][-1]):.4f}\")\n",
    "        print(f\"  Group {group} - Val Loss: {np.mean(group_val_loss_histories[group][-1]):.4f}, Val Accuracy: {np.mean(group_val_accuracy_histories[group][-1]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f5b0d-19b9-4050-b245-6d13078d0066",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class MultiStreamNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiStreamNet, self).__init__()\n",
    "\n",
    "        # Define the streams\n",
    "        self.stream_1 = resnet50(pretrained=True, progress=True, num_classes=64)  # First stream using resnet50\n",
    "        self.stream_2 = NN_beam_pred(num_features=4, num_output=64)  # Second stream using NN_beam_pred\n",
    "        \n",
    "        # Add Batch Normalization layers\n",
    "        self.bn_stream_1 = nn.BatchNorm1d(128)  # Adjust according to the output size of resnet50\n",
    "        self.bn_stream_2 = nn.BatchNorm1d(128)   # Adjust according to the output size of NN_beam_pred\n",
    "\n",
    "        # Combine the two streams and apply Multinomial Logistic Regression\n",
    "        self.fc = MultinomialLogisticRegression(input_size=256, num_classes=64)  # Adjust input_size & num_classes\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Process through stream 1 (resnet50)\n",
    "        out1 = self.stream_1(x1)\n",
    "        out2 = self.bn_stream_1(out1)\n",
    "        print(out1.shape)\n",
    "        print(out2.shape)\n",
    "        \n",
    "        # Process through stream 2 (NN_beam_pred)\n",
    "        out2 = self.stream_2(x2)\n",
    "        out2 = self.bn_stream_2(out2)\n",
    "\n",
    "        # Concatenate the outputs from both streams\n",
    "        out = torch.cat((out1, out2), dim=1)\n",
    "        \n",
    "        # Final classification layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Instantiate the model\n",
    "model = MultiStreamNet().to(device)\n",
    "\n",
    "# Set the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Example training loop\n",
    "def train(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            x, y = data  # Assuming inputs1 and inputs2 are from different streams\n",
    "            x[\"images\"], x[\"pos_height\"], y = x[\"images\"].to(device), x[\"pos_height\"].to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(x[\"images\"], x[\"pos_height\"])\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, y)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Print average loss per epoch\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Assuming `train_loader` is a DataLoader object containing the training data\n",
    "train(model, train_loaders[0], criterion, optimizer)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6e4d18-b3ff-4a6f-b813-e1db3738d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = global_rounds\n",
    "# Convert metrics to numpy arrays for easy manipulation\n",
    "group_train_loss_histories = {k: np.array(v) for k, v in group_train_loss_histories.items()}\n",
    "group_train_accuracy_histories = {k: np.array(v) for k, v in group_train_accuracy_histories.items()}\n",
    "group_val_loss_histories = {k: np.array(v) for k, v in group_val_loss_histories.items()}\n",
    "group_val_accuracy_histories = {k: np.array(v) for k, v in group_val_accuracy_histories.items()}\n",
    "\n",
    "# Handle potential one-dimensional arrays\n",
    "group_train_loss_mean = {k: v.mean(axis=1) if v.ndim > 1 else v for k, v in group_train_loss_histories.items()}\n",
    "group_train_loss_std = {k: v.std(axis=1) if v.ndim > 1 else np.zeros_like(v) for k, v in group_train_loss_histories.items()}\n",
    "group_val_loss_mean = {k: v.mean(axis=1) if v.ndim > 1 else v for k, v in group_val_loss_histories.items()}\n",
    "group_val_loss_std = {k: v.std(axis=1) if v.ndim > 1 else np.zeros_like(v) for k, v in group_val_loss_histories.items()}\n",
    "\n",
    "group_train_acc_mean = {k: v.mean(axis=1) if v.ndim > 1 else v for k, v in group_train_accuracy_histories.items()}\n",
    "group_train_acc_std = {k: v.std(axis=1) if v.ndim > 1 else np.zeros_like(v) for k, v in group_train_accuracy_histories.items()}\n",
    "group_val_acc_mean = {k: v.mean(axis=1) if v.ndim > 1 else v for k, v in group_val_accuracy_histories.items()}\n",
    "group_val_acc_std = {k: v.std(axis=1) if v.ndim > 1 else np.zeros_like(v) for k, v in group_val_accuracy_histories.items()}\n",
    "\n",
    "# Combined Plot for All Modalities\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "for group in [1, 2, 3]:\n",
    "    plt.plot(range(1, num_epochs + 1), group_train_loss_mean[group], label=f\"Group {group} Train Loss\")\n",
    "    plt.fill_between(range(1, num_epochs + 1), \n",
    "                     group_train_loss_mean[group] - group_train_loss_std[group], \n",
    "                     group_train_loss_mean[group] + group_train_loss_std[group], \n",
    "                     alpha=0.2)\n",
    "    plt.plot(range(1, num_epochs + 1), group_val_loss_mean[group], label=f\"Group {group} Validation Loss\", linestyle=\"dashed\")\n",
    "    plt.fill_between(range(1, num_epochs + 1), \n",
    "                     group_val_loss_mean[group] - group_val_loss_std[group], \n",
    "                     group_val_loss_mean[group] + group_val_loss_std[group], \n",
    "                     alpha=0.2)\n",
    "\n",
    "plt.title(\"Loss over Epochs for All Groups\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Accuracy Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "for group in [1, 2, 3]:\n",
    "    plt.plot(range(1, num_epochs + 1), group_train_acc_mean[group], label=f\"Group {group} Train Accuracy\")\n",
    "    plt.fill_between(range(1, num_epochs + 1), \n",
    "                     group_train_acc_mean[group] - group_train_acc_std[group], \n",
    "                     group_train_acc_mean[group] + group_train_acc_std[group], \n",
    "                     alpha=0.2)\n",
    "    plt.plot(range(1, num_epochs + 1), group_val_acc_mean[group], label=f\"Group {group} Validation Accuracy\", linestyle=\"dashed\")\n",
    "    plt.fill_between(range(1, num_epochs + 1), \n",
    "                     group_val_acc_mean[group] - group_val_acc_std[group], \n",
    "                     group_val_acc_mean[group] + group_val_acc_std[group], \n",
    "                     alpha=0.2)\n",
    "\n",
    "plt.title(\"Accuracy over Epochs for All Groups\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([0, 1])  # Ensure y-axis is between 0 and 1 for accuracy\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef6bc25-66fc-463b-bf3b-649b95dad7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert numpy arrays to lists for serialization\n",
    "data_to_save = {\n",
    "    \"group_train_loss_mean\": {k: v.tolist() for k, v in group_train_loss_mean.items()},\n",
    "    \"group_train_loss_std\": {k: v.tolist() for k, v in group_train_loss_std.items()},\n",
    "    \"group_val_loss_mean\": {k: v.tolist() for k, v in group_val_loss_mean.items()},\n",
    "    \"group_val_loss_std\": {k: v.tolist() for k, v in group_val_loss_std.items()},\n",
    "    \"group_train_acc_mean\": {k: v.tolist() for k, v in group_train_acc_mean.items()},\n",
    "    \"group_train_acc_std\": {k: v.tolist() for k, v in group_train_acc_std.items()},\n",
    "    \"group_val_acc_mean\": {k: v.tolist() for k, v in group_val_acc_mean.items()},\n",
    "    \"group_val_acc_std\": {k: v.tolist() for k, v in group_val_acc_std.items()}\n",
    "}\n",
    "\n",
    "with open(\"local_drone.json\", \"w\") as f:\n",
    "    json.dump(data_to_save, f)\n",
    "print(\"DSGD_metrics.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
